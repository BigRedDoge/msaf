{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add last boundary to human annotations\n",
      "import jams2\n",
      "import glob\n",
      "import json\n",
      "\n",
      "annotators = {}\n",
      "annotators[\"Colin\"] = {\n",
      "    \"name\"  : \"Colin\",\n",
      "    \"email\" : \"colin.z.hua@gmail.com\"\n",
      "}\n",
      "annotators[\"Eleni\"] = {\n",
      "    \"name\"  : \"Eleni\",\n",
      "    \"email\" : \"evm241@nyu.edu\"\n",
      "}\n",
      "annotators[\"Evan\"] = {\n",
      "    \"name\"  : \"Evan\",\n",
      "    \"email\" : \"esj254@nyu.edu\"\n",
      "}\n",
      "annotators[\"John\"] = {\n",
      "    \"name\"  : \"John\",\n",
      "    \"email\" : \"johnturner@me.com\"\n",
      "}\n",
      "annotators[\"Shuli\"] = {\n",
      "    \"name\"  : \"Shuli\",\n",
      "    \"email\" : \"luiseslt@gmail.com\"\n",
      "}\n",
      "\n",
      "def update_last_boundary(jam_file, annotator_name, time, context):\n",
      "    jam = jams2.load(jam_file)\n",
      "    for annotation in jam[\"sections\"]:\n",
      "        if annotation.annotation_metadata.annotator.name == annotator_name:\n",
      "            for i, data in enumerate(annotation.data):\n",
      "                if annotation.data[i+1].label.context == \"small_scale\":\n",
      "                    data.end.value = time\n",
      "                    break\n",
      "            break\n",
      "    json.dump(jam, open(jam_file, \"w\"), indent=2)\n",
      "    \n",
      "def add_last_boundary(jam_file, annotator_name, time, context):\n",
      "    jam = jams2.load(jam_file)\n",
      "    for annotation in jam[\"sections\"]:\n",
      "        if annotation.annotation_metadata.annotator.name == annotator_name:\n",
      "            for i, data in enumerate(annotation.data):\n",
      "                if annotation.data[i+1].label.context == \"small_scale\":\n",
      "                    segment = annotation.create_datapoint()\n",
      "                    segment.start.value = data.end.value\n",
      "                    segment.end.value = time\n",
      "                    segment.label.value = \"END\"\n",
      "                    segment.label.context = context\n",
      "                    break\n",
      "            break\n",
      "    json.dump(jam, open(jam_file, \"w\"), indent=2)\n",
      "\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "context = \"large_scale\"\n",
      "for jam_file in jam_files:\n",
      "    jam = jams2.load(jam_file)\n",
      "    dur = jam.metadata.duration\n",
      "    for key in annotators.keys():\n",
      "        inters, labels = jams2.converters.load_jams_range(jam_file, \"sections\", \n",
      "                                                          annotator_name=annotators[key][\"name\"], \n",
      "                                                          context=context)\n",
      "        if dur - inters[-1,-1] < -.1:\n",
      "            print \"Warning: the last boundary by %s is placed after the track duration! (%s)\" % \\\n",
      "                (key, jam_file)\n",
      "            print dur, inters[-1, -1]\n",
      "            update_last_boundary(jam_file, key, dur, context)\n",
      "        \n",
      "        if np.abs(dur - inters[-1, -1]) > 0.5:\n",
      "            print \"Warning: the last boundary by %s is not placed at the end of the track! (%s)\" % \\\n",
      "                (key, jam_file) \n",
      "            print dur, inters[-1, -1]\n",
      "            add_last_boundary(jam_file, key, dur, context)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {
      "slideshow": {
       "slide_type": "-"
      }
     },
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: the last boundary by Eleni is placed after the track duration! (/Users/uri/datasets/SubSegments/annotations/Epiphyte_0780_letmebereal.jams)\n",
        "182.77805 182.880362812\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Move estimations to SubSegments folder\n",
      "import glob\n",
      "import os\n",
      "import shutil\n",
      "\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "for jam_file in jam_files:\n",
      "    orig = \"/Users/uri/datasets/Segments/estimations/\" + os.path.basename(jam_file)[:-5] + \".json\"\n",
      "    dest = \"/Users/uri/datasets/SubSegments/estimations/\" + os.path.basename(orig)\n",
      "    shutil.copy(orig, dest)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 1: Dur x #bounds x score\n",
      "import sys\n",
      "import jams2\n",
      "import os\n",
      "import glob\n",
      "\n",
      "sys.path.append(\"..\")\n",
      "import eval as EV\n",
      "import msaf_io as MSAF\n",
      "\n",
      "# max_dur: 1728.257007 (SALAMI_878)\n",
      "# max #boundaries: 75 (SALAMI_888)\n",
      "N = 76\n",
      "M = 100\n",
      "# max_M = 1730.\n",
      "max_M = 2000.\n",
      "\n",
      "MAX_N = 138\n",
      "MAX_M = 149\n",
      "\n",
      "algos = [\"olda\", \"siplca\", \"serra\", \"levy\", \"foote\"]\n",
      "trim = False\n",
      "params_dict = {\"olda\" : \"\", \"siplca\" : \"\", \"serra\" : \"mix\",\n",
      "               \"levy\" : \"mfcc\" , \"foote\" : \"mfcc\"}\n",
      "bins = 250\n",
      "\n",
      "in_path = \"/Users/uri/datasets/Segments/\"\n",
      "est_files = glob.glob(os.path.join(in_path, \"estimations\", \"*.json\"))\n",
      "jam_files = glob.glob(os.path.join(in_path, \"annotations\", \"*.jams\"))\n",
      "\n",
      "\n",
      "heat_map = np.zeros((9, N, M))\n",
      "counts = np.zeros((N, M))\n",
      "SD = np.zeros((len(est_files), len(algos), 3)) # Source Data for Eric\n",
      "for i, est_file in enumerate(est_files):\n",
      "    ds_prefix = os.path.basename(est_file).split(\"_\")[0]\n",
      "\n",
      "    # Get corresponding annotation file\n",
      "    jam_file = EV.get_annotation(est_file, jam_files)\n",
      "\n",
      "    # Get number of bounds\n",
      "    try:\n",
      "        ref_inter, ref_labels = jams2.converters.load_jams_range(jam_file,\n",
      "            \"sections\", annotator=0, context=MSAF.prefix_dict[ds_prefix])\n",
      "    except:\n",
      "        print \"No annotation for %s, skipping.\" % jam_file\n",
      "        continue\n",
      "        \n",
      "    n = np.min([len(ref_inter) + 1, MAX_N])\n",
      "    \n",
      "    # Get duration\n",
      "    jam = jams2.load(jam_file)\n",
      "    dur = jam.metadata.duration\n",
      "\n",
      "    # Place duration into correct bin\n",
      "    m = np.min([int(dur / max_M * M), MAX_M])\n",
      "    \n",
      "    # Compute score\n",
      "    score = []\n",
      "    for j, algo_id in enumerate(algos):\n",
      "        params = {\"feature\" : params_dict[algo_id]}\n",
      "        est_inter = MSAF.read_boundaries(est_file, algo_id, False, **params)\n",
      "        res = EV.compute_results(ref_inter, est_inter, trim, bins, est_file)\n",
      "        score.append(res)\n",
      "        SD[i, j, :] = np.array([len(ref_inter) + 1, dur, res[2]])\n",
      "    score = np.mean(np.asarray(score), axis=0)\n",
      "    \n",
      "    # Add to heat map\n",
      "    heat_map[:, n, m] += score\n",
      "    counts[n, m] += 1\n",
      "\n",
      "print SD.shape\n",
      "np.save(open(\"exp1.npz\", \"w\"), SD)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1030.jams\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1030.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1040.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1040.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1052.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1052.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1126.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1126.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1140.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1140.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1178.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1178.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1320.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1320.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1398.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1398.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1410.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1410.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1426.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1426.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1430.jams\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1430.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1440.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1440.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1466.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1466.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1486.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1486.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_1500.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_1500.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_872.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_872.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_918.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_918.jams, skipping.\n",
        "Warning: sections empty in file /Users/uri/datasets/Segments/annotations/SALAMI_966.jams"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "No annotation for /Users/uri/datasets/Segments/annotations/SALAMI_966.jams, skipping.\n",
        "(2174, 5, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 316
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 1 (cont)\n",
      "\n",
      "metric_dict = {\"F3\" : 2, \"F05\" : 5, \"D\" : 6, \"$\\sigma_{R2E}$\" : 7,\n",
      "               \"$\\sigma_{E2R}$\" : 8}\n",
      "metric = \"$\\sigma_{E2R}$\"\n",
      "\n",
      "# Reduce data\n",
      "mN = 40\n",
      "mM = 40\n",
      "heat_map[np.isnan(heat_map)] = 0\n",
      "heat_map_metric = np.zeros((mN, mM))\n",
      "for i in xrange(heat_map[metric_dict[metric], :, :].shape[0]):\n",
      "    for j in xrange(heat_map[metric_dict[metric], :, :].shape[1]):\n",
      "        heat_map_metric[np.min([i, mN-1]), np.min([j, mM-1])] += heat_map[metric_dict[metric], i, j]\n",
      "idx = np.where(heat_map_metric == 0)\n",
      "heat_map_metric[idx] = np.nan\n",
      "\n",
      "# Plotting\n",
      "figsize = (4, 3)\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "plt.imshow(heat_map_metric, interpolation=\"nearest\", aspect=\"auto\", cmap=\"hot\")\n",
      "plt.xlabel(\"Duration (seconds)\")\n",
      "plt.ylabel(\"Number of boundaries\")\n",
      "plt.title(\"#boundaries vs duration using %s\" % metric)\n",
      "plt.gcf().subplots_adjust(bottom=0.18)\n",
      "plt.xticks(np.arange(0, mM, 10))\n",
      "plt.gca().set_xticklabels(np.arange(0, mM+1, 10, dtype=int) / float(M) * max_M)\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
        "\n",
        " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
        "\n",
        " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
        "\n",
        " ..., \n",
        " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
        "\n",
        " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
        "\n",
        " [[ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  ..., \n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
        "  [ 0.  0.  0. ...,  0.  0.  0.]]]\n"
       ]
      }
     ],
     "prompt_number": 170
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 2a: Obtain evaluation for all the different subjects\n",
      "import sys\n",
      "sys.path.append(\"..\")\n",
      "import eval as EV\n",
      "\n",
      "reload(EV)\n",
      "\n",
      "def print_results(results, std=False):\n",
      "    \"\"\"Print all the results.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    results: np.array(9)\n",
      "        Results in the following format:\n",
      "            0   :   Precision 3 seconds\n",
      "            1   :   Recall 3 seconds\n",
      "            2   :   F-measure 3 seconds\n",
      "            3   :   Precision 0.5 seconds\n",
      "            4   :   Recall 0.5 seconds\n",
      "            5   :   F-measure 0.5 seconds\n",
      "            6   :   Information Gain\n",
      "            7   :   Median Deviation from Annotated to Estimated boundary\n",
      "            8   :   Median Deviation from Estimated to Annotated boundary\n",
      "    \"\"\"\n",
      "    results = np.asarray(results)\n",
      "    res = results.mean(axis=0)\n",
      "    print \"F3: %.2f, P3: %.2f, R3: %.2f, F05: %.2f, P05: %.2f, \" \\\n",
      "                 \"R05: %.2f, D: %.4f, Ann2EstDev: %.2f, Est2AnnDev: %.2f\" % \\\n",
      "                 (100 * res[2], 100 * res[0], 100 * res[1], 100 * res[5],\n",
      "                  100 * res[3], 100 * res[4], res[6], res[7], res[8])\n",
      "\n",
      "\n",
      "in_path = \"/Users/uri/datasets/SubSegments/\"\n",
      "algos = [\"olda\", \"siplca\", \"serra\", \"levy\", \"foote\"]\n",
      "trim = False\n",
      "params_dict = {\"olda\" : \"\", \"siplca\" : \"\", \"serra\" : \"mix\",\n",
      "               \"levy\" : \"mfcc\" , \"foote\" : \"mfcc\"}\n",
      "annotators = [\"GT\", \"Colin\", \"Eleni\", \"Evan\", \"John\", \"Shuli\"]\n",
      "\n",
      "SD = np.zeros((50, 6, 5, 5)) # Tracks, annotators, Num algos, num metrics\n",
      "F3_F05 = np.zeros((50, 6, 5, 2)) # Tracks, GT+annotators, algorithms, F3 + F05\n",
      "for i, annotator in enumerate(annotators):\n",
      "    res = []\n",
      "    std = []\n",
      "    res_tot = []\n",
      "    for j, algo_id in enumerate(algos):\n",
      "        params = {\"feature\" : params_dict[algo_id]}\n",
      "        results = EV.process(in_path, algo_id, trim=trim, annotator=annotator, \n",
      "                             **params)\n",
      "        SD[:, i, j, :] = results[:, [2,5, 6, 7, 8]]\n",
      "        res_tot.append(results[:, [2,5]])\n",
      "        res.append(np.mean(results, axis=0))\n",
      "        std.append(np.std(results, axis=0))\n",
      "        F3_F05[:, i, j, :] = results[:, [2, 5]]\n",
      "    res_tot = np.asarray(res_tot)\n",
      "    print annotator\n",
      "    print_results(res)\n",
      "    print \"STD\", annotator\n",
      "    print_results(std)\n",
      "\n",
      "F3_F05 = np.asarray(F3_F05)\n",
      "print F3_F05.shape\n",
      "\n",
      "# Save Source Data for Eric-ah\n",
      "print SD.shape\n",
      "np.save(open(\"exp2a.npz\", \"w\"), SD)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "GT\n",
        "F3: 34.66, P3: 37.47, R3: 40.11, F05: 23.17, P05: 24.97, R05: 26.89, D: 0.5258, Ann2EstDev: 6.49, Est2AnnDev: 9.83\n",
        "STD GT\n",
        "F3: 15.74, P3: 21.69, R3: 16.42, F05: 12.02, P05: 15.30, R05: 13.35, D: 0.0791, Ann2EstDev: 7.34, Est2AnnDev: 9.14\n",
        "Colin"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/SALAMI_78.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/SALAMI_78.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/SALAMI_78.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/SALAMI_78.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/SALAMI_78.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "F3: 40.29, P3: 41.27, R3: 48.27, F05: 26.49, P05: 27.29, R05: 31.45, D: 0.5289, Ann2EstDev: 6.24, Est2AnnDev: 8.37\n",
        "STD Colin\n",
        "F3: 15.06, P3: 19.19, R3: 17.95, F05: 12.41, P05: 14.74, R05: 14.62, D: 0.0809, Ann2EstDev: 6.76, Est2AnnDev: 8.83\n",
        "Eleni"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "F3: 38.47, P3: 41.98, R3: 42.53, F05: 24.62, P05: 27.18, R05: 26.79, D: 0.5149, Ann2EstDev: 6.04, Est2AnnDev: 7.87\n",
        "STD Eleni\n",
        "F3: 15.10, P3: 20.79, R3: 15.93, F05: 11.70, P05: 15.57, R05: 11.91, D: 0.1028, Ann2EstDev: 6.74, Est2AnnDev: 7.87\n",
        "Evan"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "F3: 38.64, P3: 39.20, R3: 45.54, F05: 25.03, P05: 25.90, R05: 29.05, D: 0.5351, Ann2EstDev: 5.31, Est2AnnDev: 9.08\n",
        "STD Evan\n",
        "F3: 15.20, P3: 18.29, R3: 16.90, F05: 11.38, P05: 13.81, R05: 13.26, D: 0.0699, Ann2EstDev: 5.65, Est2AnnDev: 8.17\n",
        "John"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/estimations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.json\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "F3: 39.02, P3: 36.70, R3: 49.89, F05: 26.59, P05: 25.35, R05: 33.26, D: 0.5471, Ann2EstDev: 6.54, Est2AnnDev: 8.54\n",
        "STD John\n",
        "F3: 15.45, P3: 17.57, R3: 17.85, F05: 12.56, P05: 13.75, R05: 14.60, D: 0.0763, Ann2EstDev: 6.88, Est2AnnDev: 6.04\n",
        "Shuli"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "F3: 38.39, P3: 36.87, R3: 47.99, F05: 25.94, P05: 25.25, R05: 31.65, D: 0.5358, Ann2EstDev: 5.04, Est2AnnDev: 9.86\n",
        "STD Shuli\n",
        "F3: 15.25, P3: 18.05, R3: 16.79, F05: 11.84, P05: 13.55, R05: 12.64, D: 0.1058, Ann2EstDev: 5.32, Est2AnnDev: 11.24\n",
        "(50, 6, 5, 2)\n",
        "(50, 6, 5, 5)\n"
       ]
      }
     ],
     "prompt_number": 361
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 2b: Compare annotations\n",
      "import glob\n",
      "import sys\n",
      "sys.path.append(\"..\")\n",
      "sys.path.append(\"../experiment/\")\n",
      "import eval as EV\n",
      "import analyze_results as AR\n",
      "import json\n",
      "from collections import OrderedDict\n",
      "\n",
      "reload(EV)\n",
      "reload(AR)\n",
      "\n",
      "metric_dict = OrderedDict()\n",
      "metric_dict[\"F3\"] = 2 \n",
      "metric_dict[\"F05\"] = 5 \n",
      "metric_dict[\"D\"] = 6\n",
      "metric_dict[\"$\\sigma_{R2E}$\"] = 7\n",
      "metric_dict[\"$\\sigma_{E2R}$\"] = 8\n",
      "\n",
      "metric = \"F3\"\n",
      "trim = True\n",
      "N = 6 # N annotators\n",
      "X = np.empty((0, N, N))\n",
      "SD = np.zeros([50, 6, 6, 5]) # (num_tracks, num_annotators, num_annotators, metric)\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "for i, jam_file in enumerate(jam_files):\n",
      "    mma_res = AR.compute_mma_results(jam_file, AR.annotators, trim, gt=True)\n",
      "    x = np.zeros((N, N))\n",
      "    idx = np.triu_indices(N, k=1)\n",
      "    for j, met in enumerate(metric_dict):\n",
      "        x[idx] = mma_res[:, metric_dict[met]]\n",
      "        SD[i, :, :, j] = x\n",
      "    x[idx] = mma_res[:, metric_dict[metric]]\n",
      "    X = np.append(X, [x], axis=0)\n",
      "    \n",
      "# Save data for Ah-Rica\n",
      "print SD.shape\n",
      "np.save(open(\"exp2b.npz\", \"w\"), SD)\n",
      "    \n",
      "X_mean = np.mean(X, axis=0)\n",
      "# X_mean = X_mean + X_mean.T\n",
      "# np.fill_diagonal(X_mean, 1)\n",
      "idx_where = np.where(X_mean == 0)\n",
      "X_mean[idx_where] = np.mean(X_mean[idx])\n",
      "print X_mean\n",
      "vmax = np.sort(np.unique(X_mean))[-2]\n",
      "\n",
      "figsize = (4, 3)\n",
      "annots = [\"GT\", \"Ann1\", \"Ann2\", \"Ann3\", \"Ann4\", \"Ann5\"]\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "# plt.imshow(X_mean, interpolation=\"nearest\", aspect=\"auto\", cmap=\"hot\", vmin=X_mean.min(), vmax=vmax)\n",
      "plt.imshow(X_mean, interpolation=\"nearest\", aspect=\"auto\", cmap=\"hot\")\n",
      "print json.dumps(X_mean.tolist())\n",
      "plt.gca().set_xticks(np.arange(0,6))\n",
      "plt.gca().set_yticks(np.arange(0,6))\n",
      "plt.gca().set_xticklabels(annots)\n",
      "plt.gca().set_yticklabels(annots)\n",
      "# plt.title(\"Annotators Agreement for %s\" % metric)\n",
      "plt.title(\"Annotators Mutual Agreement\")\n",
      "plt.colorbar()\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/SALAMI_78.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/SALAMI_78.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/SALAMI_78.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/SALAMI_78.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "WARNING:root:Couldn't compute the Information Gain for file /Users/uri/datasets/SubSegments/annotations/SALAMI_78.jams\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(50, 6, 6, 5)\n",
        "[[ 0.58803096  0.58580553  0.60274822  0.62862279  0.54045208  0.5752257 ]\n",
        " [ 0.58803096  0.58803096  0.61170652  0.59340759  0.56479887  0.64702824]\n",
        " [ 0.58803096  0.58803096  0.58803096  0.60483826  0.52665242  0.59557561]\n",
        " [ 0.58803096  0.58803096  0.58803096  0.58803096  0.54361601  0.62365065]\n",
        " [ 0.58803096  0.58803096  0.58803096  0.58803096  0.58803096  0.57633595]\n",
        " [ 0.58803096  0.58803096  0.58803096  0.58803096  0.58803096  0.58803096]]\n",
        "[[0.5880309622668793, 0.5858055320534442, 0.6027482197457384, 0.6286227877765598, 0.5404520816680207, 0.5752256991571328], [0.5880309622668793, 0.5880309622668793, 0.6117065162573256, 0.5934075911739257, 0.564798874629181, 0.6470282354474653], [0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.6048382612398049, 0.5266524244063607, 0.5955756102325015], [0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5436160053470147, 0.623650649252582], [0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5763359456161314], [0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793, 0.5880309622668793]]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 476
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 3: compare all the annotated boundaries to come up with a more robust reference\n",
      "import glob\n",
      "import sys\n",
      "import os\n",
      "sys.path.append(\"..\")\n",
      "sys.path.append(\"../experiment/\")\n",
      "\n",
      "import eval as EV\n",
      "import analyze_results as AR\n",
      "import msaf_io as MSAF\n",
      "import json\n",
      "\n",
      "reload(EV)\n",
      "reload(AR)\n",
      "\n",
      "def create_weighted_annot(jam, weights, histo_times, jam_file):\n",
      "    # Annot\n",
      "    annot_name = \"Weighted\"\n",
      "    annot = None\n",
      "    for annotation in jam.sections:\n",
      "        if annotation.annotation_metadata.annotator.name == annot_name:\n",
      "            annotation.data = []\n",
      "            annot = annotation\n",
      "            break\n",
      "    if annot is None:\n",
      "        annot = jam.sections.create_annotation()\n",
      "        annot.annotation_metadata.origin = \"Synth\"\n",
      "        annot.annotation_metadata.annotator.name = annot_name\n",
      "        annot.annotation_metadata.annotator.email = \"\"\n",
      "    \n",
      "    bound_times = []\n",
      "    bound_weights = []\n",
      "    for i, weight in enumerate(weights):\n",
      "        if weight != 0:\n",
      "            bound_times.append(histo_times[i])\n",
      "            bound_weights.append(weights[i])\n",
      "    for i, bound in enumerate(bound_times):\n",
      "        if i == 0:\n",
      "            continue\n",
      "        section = annot.create_datapoint()\n",
      "        section.start.value = bound_times[i-1]\n",
      "        section.start.confidence = bound_weights[i-1]\n",
      "        section.end.value = bound_times[i]\n",
      "        section.end.confidence = bound_weights[i]\n",
      "        section.label.context = \"synth\"\n",
      "    \n",
      "    json.dump(jam, open(jam_file, \"w\"), indent=2)\n",
      "    \n",
      "\n",
      "def create_thres_annot(jam, thresh_bounds, thresh_conf, jam_file):\n",
      "    # Annot\n",
      "    annot_name = \"Threshold\"\n",
      "    annot = None\n",
      "    for annotation in jam.sections:\n",
      "        if annotation.annotation_metadata.annotator.name == annot_name:\n",
      "            annotation.data = []\n",
      "            annot = annotation\n",
      "            break\n",
      "    if annot is None:\n",
      "        annot = jam.sections.create_annotation()\n",
      "        annot.annotation_metadata.origin = \"Synth\"\n",
      "        annot.annotation_metadata.annotator.name = annot_name\n",
      "        annot.annotation_metadata.annotator.email = \"\"\n",
      "    \n",
      "    for i, bound in enumerate(thresh_bounds):\n",
      "        if i == 0:\n",
      "            continue\n",
      "        section = annot.create_datapoint()\n",
      "        section.start.value = thresh_bounds[i-1]\n",
      "        section.start.confidence = thresh_conf[i-1]\n",
      "        section.end.value = thresh_bounds[i]\n",
      "        section.end.confidence = thresh_conf[i]\n",
      "        section.label.context = \"synth\"\n",
      "    \n",
      "    json.dump(jam, open(jam_file, \"w\"), indent=2)\n",
      "\n",
      "    \n",
      "annotators = [\"GT\", \"Colin\", \"Eleni\", \"Evan\", \"John\", \"Shuli\"]\n",
      "histo_bins = 500\n",
      "N = len(annotators)\n",
      "X = np.empty((0, N, N))\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "for jam_file in jam_files:\n",
      "    # Get Duration\n",
      "    jam = jams2.load(jam_file)\n",
      "    dur = jam.metadata.duration\n",
      "    \n",
      "    # Compute the weighted boundaries\n",
      "    boundaries = []\n",
      "    ds_prefix = os.path.basename(jam_file).split(\"_\")[0]\n",
      "    for annot in annotators:\n",
      "        if annot == \"GT\":\n",
      "            ann_inter, ann_labels = jams2.converters.load_jams_range(jam_file,\n",
      "                \"sections\", annotator=0, context=MSAF.prefix_dict[ds_prefix])\n",
      "        else:\n",
      "            ann_inter, ann_labels = jams2.converters.load_jams_range(jam_file,\n",
      "                \"sections\", annotator_name=annot, context=\"large_scale\")\n",
      "        ann_times = EV.intervals_to_times(ann_inter)\n",
      "        histo_bounds, histo_times = np.histogram(ann_times, bins=histo_bins, range=(0, dur))\n",
      "        boundaries.append(histo_bounds)\n",
      "    \n",
      "    weights = np.mean(boundaries, axis=0)\n",
      "    weights /= weights.max()\n",
      "    \n",
      "    # Save weighted boundaries\n",
      "    create_weighted_annot(jam, weights, histo_times, jam_file)\n",
      "    \n",
      "    # Compoute the thresholded boundaries\n",
      "    L = 15\n",
      "    hann = np.hanning(L)\n",
      "    weighted_filt = np.convolve(weights, hann, mode=\"same\")\n",
      "    weighted_filt /= weighted_filt.max()\n",
      "    thresh_bounds = []\n",
      "    thresh_conf = []\n",
      "    for i, weight in enumerate(weighted_filt):\n",
      "        peak_found = False\n",
      "        if i == 0 and weighted_filt[i+1] < weight:\n",
      "            peak_found = True\n",
      "        elif i == len(weighted_filt) - 1 and weighted_filt[i-1] < weight:\n",
      "            peak_found = True\n",
      "        elif i != 0 and i != len(weighted_filt) - 1 and \\\n",
      "                weighted_filt[i-1] < weight and weight > weighted_filt[i+1]:\n",
      "            peak_found = True\n",
      "        if peak_found:\n",
      "            thresh_bounds.append(histo_times[i])\n",
      "            thresh_conf.append(weight)\n",
      "\n",
      "    # Save \n",
      "    create_thres_annot(jam, thresh_bounds, thresh_conf, jam_file)\n",
      "    \n",
      "#     plt.plot(weighted_filt)\n",
      "#     plt.show()\n",
      "#     sys.exit()\n",
      "\n",
      "                \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 214
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 3a (cont)\n",
      "# Now we already have a better reference, let's compute the numbers on the Weighted result\n",
      "import jams2\n",
      "import os\n",
      "import sys\n",
      "import mir_eval\n",
      "\n",
      "sys.path.append(\"..\")\n",
      "import eval as EV\n",
      "\n",
      "reload(jams2)\n",
      "reload(jams2.converters)\n",
      "\n",
      "\n",
      "def weighted_hit_rate(ref_inters, est_inters, weights_inters, window=3):\n",
      "    ref = EV.intervals_to_times(ref_inters)\n",
      "    est = EV.intervals_to_times(est_inters)\n",
      "    weights = EV.intervals_to_times(np.asarray(weights_inters))\n",
      "    \n",
      "    # Find matches\n",
      "    matching    = mir_eval.util.match_events(ref,est,window)\n",
      "    \n",
      "    # Apply weights\n",
      "    hits = np.zeros((len(matching)))\n",
      "    for i in xrange(len(matching)):\n",
      "        hits[i] = weights[matching[i][1]]\n",
      "    \n",
      "    # Compute the precision denominator (if hit not found, take mean of weights)\n",
      "    denom_prec = np.ones(len(est)) * np.mean(weights)\n",
      "    for i in xrange(len(matching)):\n",
      "        denom_prec[matching[i][0]] = weights[matching[i][1]]\n",
      "    \n",
      "    # Compute scores\n",
      "    precision   = np.sum(hits) / np.sum(denom_prec)\n",
      "    recall      = np.sum(hits) / np.sum(weights)\n",
      "    f           = mir_eval.util.f_measure(precision, recall)\n",
      "    \n",
      "    return precision, recall, f\n",
      "    \n",
      "\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "est_files = glob.glob(\"/Users/uri/datasets/SubSegments/estimations/*.json\")\n",
      "algos = [\"olda\", \"siplca\", \"serra\", \"levy\", \"foote\"]\n",
      "trim = False\n",
      "params_dict = {\"olda\" : \"\", \"siplca\" : \"\", \"serra\" : \"mix\",\n",
      "               \"levy\" : \"mfcc\" , \"foote\" : \"mfcc\"}\n",
      "\n",
      "F3_tot = []\n",
      "F05_tot = []\n",
      "SD = np.zeros((50, 7, 5, 2)) # tracks, annotators + merged, algorithms, F3 + F05\n",
      "for i, jam_file in enumerate(jam_files):\n",
      "    est_file = est_files[i]\n",
      "    assert os.path.basename(jam_file)[:-5] == os.path.basename(est_file[:-5])\n",
      "    ref_inters, ref_labels, ref_conf = jams2.converters.load_jams_range(jam_file,\n",
      "            \"sections\", annotator_name=\"Weighted\", context=\"synth\", confidence=True)\n",
      "    F3 = []\n",
      "    F05 = []\n",
      "    for j, algo_id in enumerate(algos):\n",
      "        params = {\"feature\" : params_dict[algo_id]}\n",
      "        est_inters = MSAF.read_boundaries(est_file, algo_id, False, **params)\n",
      "        p3, r3, f3 = weighted_hit_rate(ref_inters, est_inters, ref_conf, window=3)\n",
      "        F3.append(f3)\n",
      "        p05, r05, f05 = weighted_hit_rate(ref_inters, est_inters, ref_conf, window=0.5)\n",
      "        F05.append(f05)\n",
      "        SD[i, 6, j, :] = np.asarray([f3, f05])\n",
      "        \n",
      "    F3_tot.append(np.mean(F3))\n",
      "    F05_tot.append(np.mean(F05))\n",
      "    \n",
      "# Assign teh values for the other annotators\n",
      "SD[:, :-1, :, :] = F3_F05[:, :, :, :]\n",
      "\n",
      "# Save data for Eric guapo\n",
      "print SD.shape\n",
      "np.save(open(\"exp3a.npz\", \"w\"), SD)\n",
      "\n",
      "print \"F3 (weighted):\", np.mean(F3_tot), np.std(F3_tot)\n",
      "print \"F05 (weighted):\", np.mean(F05_tot), np.std(F05_tot)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(50, 7, 5, 2)\n",
        "F3 (weighted): 0.438662660079 0.0687507479344\n",
        "F05 (weighted): 0.284668254011 0.0983049649893\n"
       ]
      }
     ],
     "prompt_number": 363
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 3b\n",
      "# Thresholded version of merged boundaries\n",
      "import jams2\n",
      "import os\n",
      "import sys\n",
      "import mir_eval\n",
      "\n",
      "sys.path.append(\"..\")\n",
      "import eval as EV\n",
      "\n",
      "reload(jams2)\n",
      "reload(jams2.converters)\n",
      "\n",
      "jam_files = glob.glob(\"/Users/uri/datasets/SubSegments/annotations/*.jams\")\n",
      "est_files = glob.glob(\"/Users/uri/datasets/SubSegments/estimations/*.json\")\n",
      "algos = [\"olda\", \"siplca\", \"serra\", \"levy\", \"foote\"]\n",
      "trim = False\n",
      "window = 3\n",
      "params_dict = {\"olda\" : \"\", \"siplca\" : \"\", \"serra\" : \"mix\",\n",
      "               \"levy\" : \"mfcc\" , \"foote\" : \"mfcc\"}\n",
      "\n",
      "def filter_ref(ref_inters, ref_conf, th):\n",
      "    assert len(ref_inters) == len(ref_conf)\n",
      "    ref_times = EV.intervals_to_times(ref_inters)\n",
      "    conf_times = EV.intervals_to_times(ref_conf)\n",
      "    idx = np.argwhere(conf_times >= th)\n",
      "    ref_times = ref_times[idx].flatten()\n",
      "    return EV.times_to_intervals(ref_times)\n",
      "\n",
      "F3_th = []\n",
      "F05_th = []\n",
      "SD = np.zeros([50, 5, 20, 2]) # num_tracks, num_algorithms, num_thresh, F-measures\n",
      "for i, th in enumerate(np.arange(0, 1, .05)):\n",
      "    F3_tot = []\n",
      "    F05_tot = []\n",
      "    print \"computing \", th\n",
      "    j = 0\n",
      "    for jam_file, est_file in zip(jam_files, est_files):\n",
      "        assert os.path.basename(jam_file)[:-5] == os.path.basename(est_file[:-5])\n",
      "        ref_inters, ref_labels, ref_conf = jams2.converters.load_jams_range(jam_file,\n",
      "                \"sections\", annotator_name=\"Weighted\", context=\"synth\", confidence=True)\n",
      "        ref_inters = filter_ref(ref_inters, np.asarray(ref_conf), th)\n",
      "        \n",
      "        F3 = []\n",
      "        F05 = []\n",
      "        if len(ref_inters) == 0:\n",
      "            F3 = [0]\n",
      "            F05 = [0]\n",
      "            SD[j, :, i, :] = 0\n",
      "        else:\n",
      "            for k, algo_id in enumerate(algos):\n",
      "                params = {\"feature\" : params_dict[algo_id]}\n",
      "                est_inters = MSAF.read_boundaries(est_file, algo_id, False, **params)\n",
      "                p, r, f = mir_eval.boundary.detection(ref_inters, est_inters, window=3)\n",
      "                F3.append(f)\n",
      "                p, r, f = mir_eval.boundary.detection(ref_inters, est_inters, window=0.5)\n",
      "                F05.append(f)\n",
      "                SD[j, k, i, :] = np.asarray([F3[-1], F05[-1]])\n",
      "        F3_tot.append(np.mean(F3))\n",
      "        F05_tot.append(np.mean(F05))\n",
      "        j += 1\n",
      "    F3_th.append(np.mean(F3_tot))\n",
      "    F05_th.append(np.mean(F05_tot))\n",
      "    \n",
      "print SD.shape\n",
      "np.save(open(\"exp3b.npz\", \"w\"), SD)\n",
      "\n",
      "# Plot\n",
      "figsize = (4, 3)\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "plt.plot(np.arange(0, 1, .05), F3_th, \"b--\", label='$F_3$')\n",
      "plt.plot(np.arange(0, 1, .05), F05_th, \"r-\", label='$F_{0.5}$')\n",
      "plt.title(\"Thresholded aggregation\")\n",
      "plt.xlabel(\"Threshold\")\n",
      "plt.ylabel(\"Score\")\n",
      "plt.gca().legend(loc='upper right', shadow=True)\n",
      "plt.gcf().subplots_adjust(bottom=0.16, left=.15)\n",
      "plt.show()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "computing  0.0\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.05\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.1\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.15\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.2\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.25\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.3\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.35\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.4\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.45\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.5\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.55\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.6\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.65\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.7\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.75\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.8\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.85\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.9\n",
        "computing "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 0.95\n",
        "(50, 5, 20, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 360
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 4a\n",
      "import json\n",
      "from collections import OrderedDict\n",
      "\n",
      "tags = json.load(open(\"../experiment/results/merged_tags_ejh_resolved.json\", \"r\"))\n",
      "tags = OrderedDict(sorted(tags.items(), key=lambda t: t[0]))\n",
      "num_tags = []\n",
      "for i, t in enumerate(tags):\n",
      "    num_tags.append(len(np.where(np.asarray(tags[t]) != \"\")[0]))\n",
      "\n",
      "# Plot number of annotators who reported at least one track\n",
      "figsize = (6, 3)\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "# plt.axvspan(8, 10.8, color='g', alpha=0.5)\n",
      "# plt.axvspan(14, 14.8, color='g', alpha=0.5)\n",
      "# plt.axvspan(37, 37.8, color='g', alpha=0.5)\n",
      "plt.axvspan(0, 2.8, color='g', alpha=0.5)\n",
      "plt.bar(np.arange(len(num_tags)), np.sort(num_tags))\n",
      "plt.title(\"Analysis of difficulty from Human POV\")\n",
      "plt.axvspan(11, 12.8, color='g', alpha=0.5, label=\"Easy Tracks\\nfrom Machine POV\")\n",
      "plt.xlabel(\"Tracks sorted by number of annotators who reported at least one tag\")\n",
      "plt.ylabel(\"Number of annotators\\nwho reported at least one tag\", multialignment='center')\n",
      "plt.gca().legend(loc='lower right', shadow=True, prop={'size':11})\n",
      "plt.gcf().subplots_adjust(bottom=0.18)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 436
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Experiment 4b\n",
      "from collections import Counter\n",
      "\n",
      "tag_groups = [\"annotator\", \"audio_quality\", \"form\", \"instrumentation\", \"style\"]\n",
      "tag_names = np.asarray([\"Annotator\", \"Audio Quality\", \"Form\", \"Instrumentation\", \"Style\"])\n",
      "tag_frequency = np.zeros(len(tag_groups), dtype=int)\n",
      "count_tags = []\n",
      "for key, ann_tags in tags.iteritems():\n",
      "    for tag in ann_tags:\n",
      "        for real_tag in tag.split(\",\"):\n",
      "            if real_tag != \"\":\n",
      "                count_tags.append(real_tag)\n",
      "            for i, tag_group in enumerate(tag_groups):\n",
      "                if real_tag.split(\"-\")[0] in tag_group and real_tag.split(\"-\")[0] != \"\":\n",
      "                    tag_frequency[i] += 1\n",
      "\n",
      "print tag_frequency\n",
      "counter = Counter(count_tags)\n",
      "print counter\n",
      "\n",
      "# Sort tags\n",
      "idx = np.argsort(tag_frequency)\n",
      "tag_frequency = tag_frequency[idx]\n",
      "tag_names = tag_names[idx]\n",
      "\n",
      "# Plot\n",
      "figsize = (6, 3)\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "plt.barh(np.arange(len(tag_frequency)), tag_frequency, align=\"center\")\n",
      "plt.yticks(np.arange(len(tag_frequency)))\n",
      "plt.gca().set_yticklabels(tag_names)\n",
      "plt.gcf().subplots_adjust(bottom=0.17, left=0.25)\n",
      "plt.xlabel(\"Number of tags\")\n",
      "plt.title(\"Difficult Tags Grouped by Type\")\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[ 7  3 41  9 42]\n",
        "Counter({u'form-varied': 14, u'style-complex': 10, u'form-long': 10, u'form-repetitive': 9, u'annotator-clueless': 7, u'style-improvisation': 7, u'style-constant': 6, u'style-slow': 5, u' form-varied': 5, u' annotator-clueless': 5, u'style-embellishments': 5, u' style-improvisation': 5, u'instrumentation-solo': 5, u' form-long': 4, u'form-complex': 4, u' style-fast': 3, u' style-embellishments': 3, u'audio_quality-noisy': 2, u' form-complex': 2, u'style-complex(rhythm)': 2, u' style-complex': 2, u'form-short': 2, u'style-theatrical': 2, u'instrumentation-solo(voice)': 2, u'style-fast': 2, u'style-complex(harmony)': 2, u'form-long(sections)': 2, u' instrumentation-no_voice': 1, u'instrumentation-solo(guitar)': 1, u'instrumentation-no_voice': 1, u'style-jazz': 1, u' audio_quality-muffled': 1, u' form-repetitive': 1, u'audio_quality-muffled': 1, u' style-jazz': 1, u' style-complex(harmony)': 1})\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 470,
       "text": [
        "<matplotlib.text.Text at 0x119b3df10>"
       ]
      }
     ],
     "prompt_number": 470
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "COLORS = np.asarray([\"b\", \"g\", \"r\", \"m\", \"y\", \"c\"])\n",
      "ANNOTS = np.asarray([\"GT\", \"Ann1\", \"Ann2\", \"Ann3\", \"Ann4\", \"Ann5\"])\n",
      "\n",
      "SD = np.load(\"/Users/uri/Dropbox/NYU/Publications/ISMIR2014-NietoHumphreyFarboodBello/exp2b.npz\")\n",
      "num_trakcs = SD.shape[0]  #(num_tracks, num_annotators, num_annotators, metric)\n",
      "num_annots = SD.shape[1]  # or SD.shape[2]\n",
      "SD_F3 = SD[:, :, :, 0]    # Get only the F3\n",
      "print num_metrics\n",
      "\n",
      "# Copy the elements from the upper triangle to the lower\n",
      "for i in range(num_annots):\n",
      "    for j in range(num_annots):\n",
      "        SD[:, j, i] = SD[:, i, j]\n",
      "\n",
      "# Setup the figure\n",
      "figsize = (6, 3)\n",
      "plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n",
      "\n",
      "# Main loop to plot the bars\n",
      "N = 8\n",
      "for i in range(num_annots):\n",
      "    # Get indices to select for the colors and the annotators (except the current one)\n",
      "    all_idxs = np.arange(num_annots)\n",
      "    idxs = np.delete(all_idxs, i)\n",
      "    color = COLORS[idxs]\n",
      "    labels = ANNOTS[idxs]\n",
      "    \n",
      "    # Get all the means for all the annotators (except the current one)\n",
      "    annot = np.asarray([np.mean(x) for x in SD_F3.swapaxes(2,0)[idxs, i, :]])\n",
      "    \n",
      "    # Sort\n",
      "    sort_idxs = np.argsort(annot)\n",
      "#     annot = annot[sort_idxs]\n",
      "#     color = color[sort_idxs]\n",
      "#     labels = labels[sort_idxs]\n",
      "    \n",
      "    # Plot\n",
      "    if i != 0:\n",
      "        last_bar = bars[-1]\n",
      "    bars = plt.bar(np.arange(i*N, i*N+len(annot)), annot, width=1, color=color, align=\"center\")\n",
      "\n",
      "bars += (last_bar,)\n",
      "labels = tuple(labels) + (\"Ann5\",)\n",
      "# plt.legend((bars[0], bars[4], bars[2], bars[3], bars[1], bars[5]), \n",
      "#            (labels[0], labels[4], labels[2], labels[3], labels[1], labels[5]) , loc='center right')\n",
      "plt.legend(bars, labels, loc='center right')\n",
      "plt.xticks(np.arange(num_annots/2-1, N*num_annots, N))\n",
      "plt.gca().set_xticklabels(ANNOTS)\n",
      "plt.gca().set_ylim(0.45, 0.7)\n",
      "plt.gca().set_xlim(-1, N*num_annots + 18)\n",
      "plt.ylabel(\"Hit Rate F-measure\")\n",
      "plt.title(\"Agreement between Human Annotations\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 571,
       "text": [
        "<matplotlib.text.Text at 0x11a877f50>"
       ]
      }
     ],
     "prompt_number": 571
    }
   ],
   "metadata": {}
  }
 ]
}