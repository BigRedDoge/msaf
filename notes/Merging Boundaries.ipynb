{
 "metadata": {
  "name": "",
  "signature": "sha256:d0b9c6235b7f24cc8e86b02f7e2459868ac2a0fbb5302775db2eef0bb6ac6f91"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "import glob\n",
      "import os\n",
      "import sys\n",
      "import mir_eval\n",
      "from collections import OrderedDict\n",
      "\n",
      "# sys.path.append(\"..\")\n",
      "# import msaf_io as MSAF\n",
      "# import eval as EV\n",
      "from msaf import jams2\n",
      "from msaf import utils\n",
      "\n",
      "# And the tree module\n",
      "sys.path.append(\"/Users/uri/NYU/Spring14/hier_eval/\")\n",
      "import tree\n",
      "\n",
      "algos = [\"olda\", \"siplca\", \"serra\", \"levy\", \"foote\"]\n",
      "params_dict = {\"olda\" : \"\", \"siplca\" : \"\", \"serra\" : \"mix\",\n",
      "               \"levy\" : \"mfcc\" , \"foote\" : \"mfcc\"}\n",
      "annotators = OrderedDict()\n",
      "annotators[\"GT\"] = {\n",
      "    \"name\"  : \"GT\",\n",
      "    \"email\" : \"TODO\"\n",
      "}\n",
      "annotators[\"Colin\"] = {\n",
      "    \"name\"  : \"Colin\",\n",
      "    \"email\" : \"colin.z.hua@gmail.com\"\n",
      "}\n",
      "annotators[\"Eleni\"] = {\n",
      "    \"name\"  : \"Eleni\",\n",
      "    \"email\" : \"evm241@nyu.edu\"\n",
      "}\n",
      "annotators[\"Evan\"] = {\n",
      "    \"name\"  : \"Evan\",\n",
      "    \"email\" : \"esj254@nyu.edu\"\n",
      "}\n",
      "annotators[\"John\"] = {\n",
      "    \"name\"  : \"John\",\n",
      "    \"email\" : \"johnturner@me.com\"\n",
      "}\n",
      "annotators[\"Shuli\"] = {\n",
      "    \"name\"  : \"Shuli\",\n",
      "    \"email\" : \"luiseslt@gmail.com\"\n",
      "}\n",
      "\n",
      "dataset_path = \"/Users/uri/datasets/SubSegments/annotations/\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# SHAG functions (copy pasted)\n",
      "\n",
      "def round_time(t, res=0.1):\n",
      "    v = int(t / float(res)) * res\n",
      "    return v\n",
      "\n",
      "def create_jams(boundaries, out_file):\n",
      "    jam = jams2.Jams()\n",
      "    jam.metadata.duration = boundaries[0][-1]\n",
      "    sections = jam.sections.create_annotation()\n",
      "    for c, boundary_level in enumerate(boundaries):\n",
      "        b_intervals = zip(boundary_level[:], boundary_level[1:])\n",
      "        for b_interval in b_intervals:\n",
      "            section = sections.create_datapoint()\n",
      "            section.start.value = b_interval[0]\n",
      "            section.end.value = b_interval[1]\n",
      "            section.label.context = str(c)\n",
      "    with open(out_file, \"w\") as f:\n",
      "        json.dump(jam, f, indent=2)\n",
      "        \n",
      "def draw_tree(T):\n",
      "    H = lca_matrix(T)\n",
      "    n = len(H)\n",
      "    \n",
      "    # Find the grid points\n",
      "    idx = np.squeeze(np.argwhere(np.sum(np.abs(np.diff(H, axis=0)), axis=1) > 0)) + 1\n",
      "    \n",
      "    plt.imshow(H, aspect='equal', interpolation='nearest', cmap='hot_r', extent=[0, len(H), len(H), 0])\n",
      "    \n",
      "    plt.vlines(idx, 0, n, colors='k', alpha=0.15)\n",
      "    plt.hlines(idx, 0, n, colors='k', alpha=0.15)\n",
      "    \n",
      "    sr = 10\n",
      "    plt.xticks(np.arange(0, len(H), len(H)/4), np.arange(0, len(H), len(H)/4) / float(10))\n",
      "    plt.yticks(np.arange(0, len(H), len(H)/4), np.arange(0, len(H), len(H)/4) / float(10))\n",
      "    plt.xlabel('Duration (seconds)')\n",
      "    plt.show()\n",
      "    \n",
      "def lca_matrix(tree, res=0.1):\n",
      "    '''\n",
      "    Input: a segment tree\n",
      "    Output: an n-by-n integer matrix indicating the height of the least common ancestor of each pair of frames (i, j)\n",
      "    '''\n",
      "    \n",
      "    # Figure out how many frames we need\n",
      "    n = int((round_time(tree.root.segment.end, res=res) - round_time(tree.root.segment.start, res=res) ) / res)\n",
      "    \n",
      "    # Build a mapping of level->height\n",
      "    height_map = {}\n",
      "    \n",
      "    # Initialize the LCA matrix\n",
      "    H = np.zeros( (n, n), dtype=np.uint8)\n",
      "    \n",
      "    # Breadth-first traversal of the tree\n",
      "    queue = [tree.root]\n",
      "    while queue:\n",
      "        node = queue.pop(0)\n",
      "        \n",
      "        # Get the node's level\n",
      "        if node.parent is not None:\n",
      "            height_map[node] = 1 + height_map[node.parent]\n",
      "        else:\n",
      "            height_map[node] = 0\n",
      "        \n",
      "        s = int(round_time(node.segment.start, res=res) / res)\n",
      "        t = int(round_time(node.segment.end, res=res) / res)\n",
      "        \n",
      "        H[s:t, s:t] = height_map[node]\n",
      "        \n",
      "        queue.extend(node.children)\n",
      "\n",
      "    return H"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_nearest(array, value):\n",
      "    idx = (np.abs(array-value)).argmin()\n",
      "    return array[idx]\n",
      "\n",
      "def adjust_boundaries(all_bounds):\n",
      "    \"\"\"Adjusts the large scale boundaries to the small scale ones.\"\"\"\n",
      "    small = np.asarray(all_bounds[0])\n",
      "    large = np.asarray(all_bounds[1])\n",
      "    for i, val in enumerate(large):\n",
      "        large[i] = find_nearest(small, val)\n",
      "    return [small] + [large]\n",
      "\n",
      "def get_boundaries(jam_file, annotator_name, context):\n",
      "    \"\"\"Gets the boundaries of a specific annotator and context.\"\"\"\n",
      "    print jam_file, annotator_name, context\n",
      "    ann_inters, ann_labels = jams2.converters.load_jams_range(jam_file,\n",
      "        \"sections\", annotator_name=annotator_name, context=context)\n",
      "    bounds = utils.intervals_to_times(ann_inters)\n",
      "    return bounds\n",
      "    \n",
      "def get_all_flat_boundaries(jam_file, ann_context=\"large_scale\", with_gt=False):\n",
      "    \"\"\"Gets all the boundaries for a given jam_file, with a specific context.\"\"\"\n",
      "    all_bounds = []\n",
      "    if with_gt:\n",
      "        start = 0\n",
      "    else:\n",
      "        start = 1\n",
      "    for i in xrange(start, len(annotators.keys())):\n",
      "        bounds = get_boundaries(jam_file, annotators.keys()[i], ann_context)\n",
      "        all_bounds.append(bounds)\n",
      "    return all_bounds\n",
      "\n",
      "def get_all_hier_boundaries(jam_file):\n",
      "    \"\"\"Gets all the boundaries for a given jam_file, large and small scale.\"\"\"\n",
      "    all_bounds = []\n",
      "    for i in xrange(1, len(annotators.keys())):\n",
      "        large_bounds = get_boundaries(jam_file, annotators.keys()[i], \"large_scale\")\n",
      "        small_bounds = get_boundaries(jam_file, annotators.keys()[i], \"small_scale\")\n",
      "        \n",
      "        # Build hierarchy\n",
      "        hier_bounds = np.asarray([small_bounds.tolist()] + [large_bounds.tolist()])\n",
      "        hier_bounds = adjust_boundaries(hier_bounds)\n",
      "        create_jams(hier_bounds, \"test/tmp.jams\")\n",
      "#         T = tree.SegmentTree(\"test/tmp.jams\", annotation_id=0)\n",
      "#         draw_tree(T)\n",
      "        \n",
      "        all_bounds.append(hier_bounds)\n",
      "    return all_bounds\n",
      "\n",
      "def get_duration(jam_file):\n",
      "    \"\"\"Returns the duration of a file.\"\"\"\n",
      "    jam = jams2.load(jam_file)\n",
      "    return float(jam.metadata.duration)\n",
      "\n",
      "def plot_boundaries(all_bounds):\n",
      "    \"\"\"Plots all the boundaries in all_bounds.\"\"\"\n",
      "    plt.figure(figsize=(6,4))\n",
      "    N = float(len(all_bounds))\n",
      "    for i, bounds in enumerate(all_bounds):\n",
      "        for bound in bounds:\n",
      "            plt.axvline(bound, ymin=i/N, ymax=(i+1)/N)\n",
      "        plt.axhline(i/N, color=\"gray\")\n",
      "    plt.title(\"All Boundaries\")\n",
      "    plt.yticks(np.arange(1, 10, 2) * 0.1)\n",
      "    plt.gca().set_yticklabels([\"Ann_%d\" % i for i in xrange(len(all_bounds))])\n",
      "    plt.show()\n",
      "\n",
      "def build_hier_bounds(weights, times, N=5):\n",
      "    \"\"\"Builds the hierarchical boundaries given the weights and the times\"\"\"\n",
      "    hier = []\n",
      "    for i in xrange(N):\n",
      "        n = (i+1)/float(N)\n",
      "        idxs = np.where(weights >= n)\n",
      "        hier.append(times[idxs].tolist())\n",
      "    return hier"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def merge_down(all_bounds, duration, win=3, res=100):\n",
      "    \"\"\"Merges multiple flat boundaries into a single flat annotation.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    all_bounds: list\n",
      "        List of np.arrays representing the boundaries in seconds.\n",
      "    duration: float\n",
      "        Duration in seconds of the entire track.\n",
      "    win: float\n",
      "        Window in seconds to merge the boundaries.\n",
      "    res: int\n",
      "        Resolution of the histogram.\n",
      "    \"\"\"\n",
      "    hmax = int(np.ceil(duration / float(res)) * res)\n",
      "    bins = int(hmax / float(win))\n",
      "    count = np.zeros(bins)\n",
      "    for bounds in all_bounds:\n",
      "        bounds = bounds[1:-1]  # remove first and last bound\n",
      "        curr_count, bins = np.histogram(bounds, bins=bins, range=(0, hmax))\n",
      "        count += curr_count\n",
      "    count /= float(len(all_bounds))\n",
      "    \n",
      "    # Remove zeroes\n",
      "    zeros_idx = np.argwhere(count == 0)\n",
      "    bounds_weights = np.delete(count, zeros_idx)\n",
      "    bounds_times = np.delete(bins, zeros_idx)[:-1] + win/2.0 # Center times\n",
      "    \n",
      "    # First and last boundaries\n",
      "    bounds_weights = np.concatenate(([1.0], bounds_weights, [1.0]))\n",
      "    bounds_times = np.concatenate(([0.0], bounds_times, [duration]))\n",
      "    return bounds_weights, bounds_times\n",
      "\n",
      "def merge_flat_to_flat(all_bounds, duration, win=3, res=100):\n",
      "    \"\"\"Merges multiple flat boundaries into a single flat annotation.\"\"\"\n",
      "    bounds_weights, bounds_times = merge_down(all_bounds, duration, win, res)\n",
      "    # TODO: Low pass filter?\n",
      "    return bounds_weights, bounds_times\n",
      "\n",
      "def merge_flat_to_hier(all_bounds, duration, win=3, res=100):\n",
      "    \"\"\"Merges multiple flat boundaries into a single hierarchical annotation.\"\"\"\n",
      "    bounds_weights, bounds_times = merge_down(all_bounds, duration, win, res)\n",
      "    # TODO: Low pass filter?\n",
      "    N = len(all_bounds)\n",
      "    hier = build_hier_bounds(bounds_weights, bounds_times, N)\n",
      "    create_jams(hier, \"test/tmp.jams\")\n",
      "    T = tree.SegmentTree(\"test/tmp.jams\", annotation_id=0)\n",
      "#     draw_tree(T)\n",
      "    return T        \n",
      "\n",
      "def merge_hier_to_flat(all_hier_bounds, duration, win=3, res=100):\n",
      "    \"\"\"Merges multiple hierarchical 2-level boundaries into a single flat annotation.\"\"\"\n",
      "    all_bounds = []\n",
      "    for hier_bounds in all_hier_bounds:\n",
      "        all_bounds.append(hier_bounds[0])   # Small scale\n",
      "        all_bounds.append(hier_bounds[1])   # Large scale\n",
      "    bounds_weights, bounds_times = merge_down(all_bounds, duration, win, res)\n",
      "    # TODO: Low pass filter?\n",
      "    return bounds_weights, bounds_times\n",
      "\n",
      "def merge_hier_to_hier(all_hier_bounds, duration, win=3, res=100):\n",
      "    \"\"\"Merges multiple hierarchical 2-level boundaries into a single hierarchical annotation.\"\"\"\n",
      "    all_bounds = []\n",
      "    for hier_bounds in all_hier_bounds:\n",
      "        all_bounds.append(hier_bounds[0])   # Small scale\n",
      "        all_bounds.append(hier_bounds[1])   # Large scale\n",
      "    bounds_weights, bounds_times = merge_down(all_bounds, duration, win, res)\n",
      "    # TODO: Low pass filter?\n",
      "    N = len(all_bounds)\n",
      "    hier = build_hier_bounds(bounds_weights, bounds_times, N)\n",
      "    create_jams(hier, \"test/tmp.jams\")\n",
      "    T = tree.SegmentTree(\"test/tmp.jams\", annotation_id=0)\n",
      "#     draw_tree(T)\n",
      "    return T "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jam_file = dataset_path + \"Epiphyte_0220_promiscuous.jams\"  # Easy track\n",
      "# jam_file = dataset_path + \"Cerulean_Bob_Dylan-Hurricane.jams\" # Harder\n",
      "# jam_file = dataset_path + \"Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\" # Harder,\n",
      "context = \"large_scale\"\n",
      "all_flat_bounds = get_all_flat_boundaries(jam_file, context, with_gt=True)\n",
      "print all_flat_bounds\n",
      "plot_boundaries(all_flat_bounds)\n",
      "# print all_flat_bounds\n",
      "duration = get_duration(jam_file)\n",
      "# bounds_weights, bounds_times = merge_flat_to_flat(all_flat_bounds, duration, win=3)\n",
      "# print bounds_weights\n",
      "# print bounds_times\n",
      "# plt.scatter(bounds_times, bounds_weights)\n",
      "# plt.show()\n",
      "# T = merge_flat_to_hier(all_flat_bounds, duration, win=3)\n",
      "# draw_tree(T)\n",
      "\n",
      "all_hier_bounds = get_all_hier_boundaries(jam_file)\n",
      "bounds_weights, bounds_times = merge_hier_to_flat(all_hier_bounds, duration, win=3)\n",
      "print bounds_weights\n",
      "print bounds_times\n",
      "print len(bounds_weights), len(bounds_times)\n",
      "plt.scatter(bounds_times, bounds_weights)\n",
      "plt.show()\n",
      "T = merge_hier_to_hier(all_hier_bounds, duration, win=3)\n",
      "draw_tree(T)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "/Users/uri/datasets/SubSegments/annotations/Epiphyte_0220_promiscuous.jams GT large_scale\n"
       ]
      },
      {
       "ename": "IndexError",
       "evalue": "index -1 is out of bounds for axis 0 with size 0",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-20-d04237259b8a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# jam_file = dataset_path + \"Cerulean_Boston_Symphony_Orchestra_&_Charles_Munch-Sympho.jams\" # Harder,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"large_scale\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mall_flat_bounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_flat_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjam_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_gt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mall_flat_bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplot_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_flat_bounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-18-3502042670a6>\u001b[0m in \u001b[0;36mget_all_flat_boundaries\u001b[0;34m(jam_file, ann_context, with_gt)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_boundaries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjam_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mann_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mall_bounds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbounds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mall_bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-18-3502042670a6>\u001b[0m in \u001b[0;36mget_boundaries\u001b[0;34m(jam_file, annotator_name, context)\u001b[0m\n\u001b[1;32m     16\u001b[0m     ann_inters, ann_labels = jams2.converters.load_jams_range(jam_file,\n\u001b[1;32m     17\u001b[0m         \"sections\", annotator_name=annotator_name, context=context)\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintervals_to_times\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann_inters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/uri/NYU/Dissertation/msaf/utils.pyc\u001b[0m in \u001b[0;36mintervals_to_times\u001b[0;34m(inters)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mset\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: index -1 is out of bounds for axis 0 with size 0"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Evaluation of merged boundaries\n",
      "\n",
      "def weighted_hit_rate(ref_inters, est_inters, weights_inters, trim=False, window=3):\n",
      "    ref = EV.intervals_to_times(ref_inters)\n",
      "    est = EV.intervals_to_times(est_inters)\n",
      "    weights = EV.intervals_to_times(np.asarray(weights_inters))\n",
      "    \n",
      "    if trim:\n",
      "        ref = ref[1:-1]\n",
      "        est = est[1:-1]\n",
      "        weights = weights[1:-1]\n",
      "    \n",
      "    # Find matches\n",
      "    matching    = mir_eval.util.match_events(ref,est,window)\n",
      "    \n",
      "    # Apply weights\n",
      "    hits = np.zeros((len(matching)))\n",
      "    for i in xrange(len(matching)):\n",
      "        hits[i] = weights[matching[i][1]]\n",
      "    \n",
      "    # Compute the precision denominator (if hit not found, take mean of weights)\n",
      "    denom_prec = np.ones(len(est)) * np.mean(weights)\n",
      "    for i in xrange(len(matching)):\n",
      "        denom_prec[matching[i][0]] = weights[matching[i][1]]\n",
      "    \n",
      "    # Compute scores\n",
      "    precision   = np.sum(hits) / np.sum(denom_prec)\n",
      "    recall      = np.sum(hits) / np.sum(weights)\n",
      "    f           = mir_eval.util.f_measure(precision, recall)\n",
      "    \n",
      "    return precision, recall, f\n",
      "\n",
      "def shag(T_ref, T_est, res=0.1, window=None, transitive=True):\n",
      "    \n",
      "    # First, build the LCA matrices\n",
      "    H_ref = lca_matrix(T_ref, res=res)\n",
      "    H_est = lca_matrix(T_est, res=res)\n",
      "    \n",
      "    # Make sure we have the right number of frames\n",
      "    assert H_ref.shape == H_est.shape\n",
      "    \n",
      "    # How many frames?\n",
      "    n = H_ref.shape[0]\n",
      "    \n",
      "    # By default, the window covers the entire track\n",
      "    if window is None:\n",
      "        window = n\n",
      "    \n",
      "    # Initialize the score\n",
      "    score = 0.0\n",
      "    \n",
      "    # Iterate over query frames\n",
      "    n_f = 0\n",
      "    \n",
      "    for q in range(n):\n",
      "        \n",
      "        # Find all pairs i,j such that H_ref[q, i] > H_ref[q, j]\n",
      "        R = H_ref[q, max(0, q-window):min(n, q+window)]\n",
      "        \n",
      "        # And the same for the estimation\n",
      "        E = H_est[q, max(0, q-window):min(n, q+window)]\n",
      "        \n",
      "        if transitive:\n",
      "            # Transitive: count comparisons across any level\n",
      "            S_ref = np.greater.outer(R, R)\n",
      "        else:\n",
      "            # Non-transitive: count comparisons only across immediate levels\n",
      "            S_ref = np.equal.outer(R, R+1)\n",
      "            \n",
      "            \n",
      "        S_est = np.greater.outer(E, E)\n",
      "        \n",
      "        # Don't count (q,q) as a result\n",
      "        idx = min(q, window)\n",
      "        S_ref[idx, :] = False\n",
      "        S_ref[:, idx] = False\n",
      "        \n",
      "        \n",
      "        # Compute normalization constant\n",
      "        Z = float(S_ref.sum())\n",
      "        \n",
      "        # Add up agreement for frames\n",
      "        if Z > 0:\n",
      "            score += np.sum(np.logical_and(S_ref, S_est)) / Z\n",
      "            n_f += 1.0\n",
      "    \n",
      "    if n_f:\n",
      "        return score / n_f\n",
      "    \n",
      "    # Convention: 0/0 = 0\n",
      "    return score\n",
      "\n",
      "def compute_weighted(merged_times, est_inters, merged_weights, trim, window=3):\n",
      "    merged_inters = EV.times_to_intervals(merged_times)\n",
      "    merged_weight_inters = EV.times_to_intervals(merged_weights)\n",
      "    p, r, f = weighted_hit_rate(merged_inters, est_inters, merged_weight_inters, trim, window=window)\n",
      "    return p, r, f\n",
      "\n",
      "def compute_shags(ref_T, est_T, window=None, transitive=True):\n",
      "    H_u = shag(ref_T, est_T, window=window, transitive=transitive)\n",
      "    H_o = shag(est_T, ref_T, window=window, transitive=transitive)\n",
      "    if H_u + H_o == 0:\n",
      "        return H_u, H_o, 0\n",
      "    return H_u, H_o, 2 * H_u * H_o / float(H_u + H_o)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import itertools\n",
      "\n",
      "annotations_path = \"/Users/uri/datasets/SubSegments/annotations/\"\n",
      "estimations_path = \"/Users/uri/datasets/SubSegments/estimations/\"\n",
      "\n",
      "annotations = glob.glob(annotations_path + \"*.jams\")\n",
      "estimations = glob.glob(estimations_path + \"*.json\")\n",
      "\n",
      "assert len(annotations) == len(estimations)\n",
      "\n",
      "results = np.zeros((len(algos), 50, 12, 10)) # 5 Algos, 50 tracks, 3 merged1 + 3 merged2 + 3 merged3 + 3 merged4 = 12, 5 sets\n",
      "context = \"large_scale\"\n",
      "trim = True\n",
      "\n",
      "def compute_evaluations(merges, est_T):\n",
      "    hier_win = 30\n",
      "    transitive = True\n",
      "    win = 3\n",
      "    res = np.zeros(12)\n",
      "    res[:3] = compute_weighted(merges[\"f2f_times\"], est_inters, merges[\"f2f_weights\"], trim, window=win)\n",
      "    res[3:6] = compute_shags(merges[\"f2h_T\"], est_T, window=hier_win, transitive=transitive)\n",
      "    res[6:9] = compute_weighted(merges[\"h2f_times\"], est_inters, merges[\"h2f_weights\"], trim, window=win)\n",
      "    res[9:] = compute_shags(merges[\"h2h_T\"], est_T, window=hier_win, transitive=transitive)\n",
      "    return res\n",
      "\n",
      "def compute_merged_bounds(annot_file, annot_idxs):\n",
      "    merges = {}\n",
      "    \n",
      "    # Flats\n",
      "    all_flat_bounds = get_all_flat_boundaries(annot_file, context)\n",
      "    duration = get_duration(annot_file)\n",
      "    all_flat_bounds = list( all_flat_bounds[i] for i in annot_idxs )\n",
      "    merges[\"f2f_weights\"], merges[\"f2f_times\"] = merge_flat_to_flat(all_flat_bounds, duration, win=3)\n",
      "    merges[\"f2h_T\"] = merge_flat_to_hier(all_flat_bounds, duration, win=3)\n",
      "    \n",
      "    # Hiers\n",
      "    all_hier_bounds = get_all_hier_boundaries(jam_file)\n",
      "    all_hier_bounds = list( all_hier_bounds[i] for i in annot_idxs )\n",
      "    merges[\"h2f_weights\"], merges[\"h2f_times\"] = merge_hier_to_flat(all_hier_bounds, duration, win=3)\n",
      "    merges[\"h2h_T\"] = merge_hier_to_hier(all_hier_bounds, duration, win=3)\n",
      "    \n",
      "    return merges\n",
      "    \n",
      "def print_evaluation(evals):\n",
      "    print \"Flat to Flat\", evals[:3]\n",
      "    print \"Flat to Hier\", evals[3:6]\n",
      "    print \"Hier to Flat\", evals[6:9]\n",
      "    print \"Hier to Hier\", evals[9:]\n",
      "\n",
      "\n",
      "for j, algo_id in enumerate(algos):\n",
      "    print \"Evaluating algorithm:\", algo_id\n",
      "    for k, annot_idx in enumerate(itertools.combinations(range(5), 2)):\n",
      "#     for k, annot_idx in enumerate(itertools.combinations(range(5), 3)):\n",
      "#     for k, annot_idx in enumerate(itertools.combinations(range(5), 4)):\n",
      "#     for annot_idx in itertools.combinations(range(5), 4):\n",
      "        print annot_idx\n",
      "        for i, (annot_file, est_file) in enumerate(zip(annotations[:], estimations[:])):\n",
      "            assert os.path.basename(annot_file)[:-4] == os.path.basename(est_file)[:-4]\n",
      "#             print \"Computing: %s\" % annot_file\n",
      "            \n",
      "            ### Merges ###\n",
      "            merges = compute_merged_bounds(annot_file, annot_idx)\n",
      "            \n",
      "            ### Evaluations ###\n",
      "            annot = \"GT\"  \n",
      "            params = {\"feature\" : params_dict[algo_id]}\n",
      "            est_inters = MSAF.read_estimations(est_file, algo_id, False, **params)\n",
      "        #     ds_prefix = os.path.basename(annot_file).split(\"_\")[0]\n",
      "        #     if annot == \"GT\":\n",
      "        #         ann_inter, ann_labels = jams2.converters.load_jams_range(annot_file,\n",
      "        #             \"sections\", annotator=0, context=MSAF.prefix_dict[ds_prefix])\n",
      "        #     else:\n",
      "        #         ann_inter, ann_labels = jams2.converters.load_jams_range(annot_file,\n",
      "        #             \"sections\", annotator_name=annot, context=\"large_scale\")\n",
      "        #     res = EV.compute_results(ann_inter, est_inters, trim, 250, est_file)\n",
      "        #     # res: [P3, R3, F3, P05, R05, F05, D, ann_to_est, est_to_ann]\n",
      "        #     print \"Standard:\\t\", res[0], res[1], res[2]\n",
      "            \n",
      "            # Create tree from flat estimated boundaries\n",
      "            duration = get_duration(annot_file)\n",
      "            est_inters[-1][-1] = np.min([est_inters[-1][-1], duration])\n",
      "            create_jams([EV.intervals_to_times(est_inters)], \"test/tmp.jams\")\n",
      "            est_T = tree.SegmentTree(\"test/tmp.jams\", annotation_id=0)\n",
      "            \n",
      "            # Compute all the evaluations\n",
      "            res = compute_evaluations(merges, est_T)\n",
      "            #print_evaluation(res)\n",
      "            \n",
      "            results[j, i, :, k] = res\n",
      "\n",
      "np.save(open(\"results_merged-pairs.npy\", \"w\"), results)\n",
      "\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Evaluating algorithm: olda\n",
        "(0, 1)\n",
        "(0, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(0, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(0, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(3, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Evaluating algorithm:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " siplca\n",
        "(0, 1)\n",
        "(0, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(0, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(0, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(3, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Evaluating algorithm:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " serra\n",
        "(0, 1)\n",
        "(0, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(0, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(0, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(3, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Evaluating algorithm:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " levy\n",
        "(0, 1)\n",
        "(0, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(0, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(0, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(3, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Evaluating algorithm:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " foote\n",
        "(0, 1)\n",
        "(0, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(0, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(0, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 2)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(1, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 3)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(2, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(3, 4)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}