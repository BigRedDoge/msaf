% -----------------------------------------------
% Template for ISMIR 2014
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2014,amsmath,cite}
\usepackage{graphicx}
\usepackage{amsfonts}

% Title.
% ------
\title{Analysis of Music Segmentation Boundaries:\\ Machines vs Humans}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
  %{First author} {School \\ Department}
  %{Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
  The automatic identification of segment boundaries in music has had a strong relevance in the field of Music Information Retrieval in the recent years.
  This task presents many challenges, and arguably the most notable one is the subjectivity of the task itself: two humans do not always agree on the same set of boundaries.
  In this work we present an analysis of this task from two different points of view: machines and humans.
  Five algorithms were selected to segment a dataset of more than 2000 tracks.
  Then we collected multiple human annotations for the 45 hardest tracks and the 5 easiest tracks from a machine point of view.
  The results show that humans' mutual agreement is much higher than machines'.
  We explore the hardest tracks from a machine point of view and the easiest tracks from a human point of view in order to identify the set of audio properties that could potentially improve the current boundary identification algorithms.
  %We present a comparison that aims to shed light into the shortcomings of the current evaluation metrics and discuss a list of properties in the audio that make this task particularly difficult from a machine point of view while humans can 
  
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

Music segmentation intro: focusing on boundaries.

Subjectivity of the task.

Analysis of how machines do vs humans. Aim to improve automatic algorithms.

Organization of the paper.

\section{Boundaries Evaluation Metrics}

The boundaries of a given track are usually evaluated using the F-measure. In this section we review the F-measure, discuss its shortcomings, and propose other metrics that might help addressing our project.

\subsection{F-measure}

The F-measure for boundaries evaluation is the most common and established metric for this task (implemented in MIREX) and it is traditionally used with a specific time window of 3 or 0.5 seconds\cite{Ong2005}. 

This metric considers an estimated boundary as \emph{correct} (a hit) if it falls under the specified time window centered at its corresponding annotated boundary.
These hits are then used to compute the Precision (number of hits over the number of estimated boundaries), and the Recall (number of hits over the number of annotated boundaries).
The F-measure is then computed as:

\begin{equation}
  F = 2 \frac{P R}{P + R}
\end{equation}

Trim\cite{Nieto2013}.
F-measure at 3 seconds and 0.5. Trim/No trim. Information Gain. Entropy scores.

\subsection{To Trim or Not To Trim}

Trimming the first and last boundaries may be the logical thing to do.

\section{Machines Identifying Boundaries}\label{sec:eval_desc}

Different approaches: novelty, homogeneity, repetition.

Choosing the five algorithms: OLDA\cite{McFee2014}, Serr\`a\cite{Serra2013},
Foote\cite{Foote1999}, Levy\cite{Levy2008}, SI-PLCA\cite{Weiss2011}.

Talk about the Mean Mutual Agreement and Mean Performance Ground-truth\cite{Holzapfel2012}.

\subsection{Music Segmentation Framework}

Open source project\footnote{Not displayed for revision process.}

Machines.

\subsection{Dataset}

Isophonics, SALAMI\cite{Smith2011}, Cerulean, Ephiphyte.

\subsection{Selection of Hardest and Easiest Tracks}

Plots of MGP and MMA.

\section{Humans Identifying Boundaries}\label{sec:using_method}

Experiment. 5 subjects. 50 tracks: 45 hard, 5 easy.

\subsection{Experiment Setup}

Talk about the SALAMI guidelines.

\section{Machines vs Humans}

Plot comparing MGP-machines vs MGP-humans.

Analyze the answers, display most common problems in a table.

%\begin{table}
 %\begin{center}
 %\begin{tabular}{|l|l|}
  %\hline
  %String value & Numeric value \\
  %\hline
  %Hello ISMIR  & 2014 \\
  %\hline
 %\end{tabular}
%\end{center}
 %\caption{Table captions should be placed below the table.}
 %\label{tab:example}
%\end{table}

%\begin{figure}
 %\centerline{\framebox{
 %\includegraphics[width=\columnwidth]{figure.png}}}
 %\caption{Figure captions should be placed below the figure.}
 %\label{fig:example}
%\end{figure}

\section{Conclusions}

Identified some audio properties that could improve the automatic segmentation algorithms.


%\begin{thebibliography}{citations}

%\bibitem {Author:00}
%E. Author:
%``The Title of the Conference Paper,''
%{\it Proceedings of the International Symposium
%on Music Information Retrieval}, pp.~000--111, 2000.

%\bibitem{Someone:10}
%A. Someone, B. Someone, and C. Someone:
%``The Title of the Journal Paper,''
%{\it Journal of New Music Research},
%Vol.~A, No.~B, pp.~111--222, 2010.

%\bibitem{Someone:04} X. Someone and Y. Someone: {\it Title of the Book},
    %Editorial Acme, Porto, 2012.

%\end{thebibliography}

\bibliography{references}

\end{document}
