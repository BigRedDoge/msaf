% -----------------------------------------------
% Template for ISMIR 2014
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2014,amsmath,cite}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tablefootnote}

% Title.
% ------
\title{Analysis of Music Segmentation Boundaries:\\ Machines vs Humans}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
  %{First author} {School \\ Department}
  %{Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
  One of the biggest challenges in the automatic identification of segment boundaries in music arises from the subjectivity of the task itself: humans do not always agree on the same set of boundaries.
  In this work we present an analysis of this task from two different points of view: machines and humans.
  Five algorithms were selected to segment a dataset of more than 2000 tracks.
  Then we collected multiple human annotations for the 45 hardest tracks and the 5 easiest tracks from a machine point of view.
  The results show that humans' mutual agreement is much higher than machines'.
  We explore the hardest tracks from a machine point of view and the easiest tracks from a human point of view in order to identify the set of audio properties that could potentially improve the current boundary identification algorithms.
  %We present a comparison that aims to shed light into the shortcomings of the current evaluation metrics and discuss a list of properties in the audio that make this task particularly difficult from a machine point of view while humans can 
  
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

TODO.
The automatic identification of segment boundaries in music has had a strong relevance in the field of Music Information Retrieval in the recent years.
This task presents many challenges, and arguably the most notable one is the subjectivity of the task itself: two humans do not always agree on the same set of boundaries.
Music segmentation intro: focusing on boundaries.

Subjectivity of the task.

Analysis of how machines do vs humans. Aim to improve automatic algorithms.

Organization of the paper.

\section{Boundaries Evaluation Metrics}\label{sec:evalmetrics}

The boundaries of a given track are usually evaluated using the hit rate metric.
In this section we review this evaluation, discuss its shortcomings, and explore other metrics that might help addressing our goals.
We refer the human annotated boundaries as \emph{reference}, and the ones identified by an algorithm as \emph{estimated}.

\subsection{Hit Rate}\label{subsec:hitrate}

The hit rate for boundaries evaluation is the most common and established metric for this task and it is traditionally used with a specific time window of 3 and/or 0.5 seconds\cite{Ong2005}. 

This metric considers an estimated boundary as \emph{correct} (a hit) if it falls under the specified time window centered at its corresponding reference boundary.
These hits are then used to compute three scores: the Precision ($P$, number of hits over the number of estimated boundaries), the Recall ($R$, number of hits over the number of reference boundaries), and the F-measure (harmonic mean between Precision and Recall), which is computed as follows:

\begin{equation}
  F = 2 \frac{P R}{P + R}
\end{equation}

\subsection{Median Deviations}

This metric involves the computation of two median deviations: \emph{median reference to estimation} ($\sigma_{R2E}$) is the median in seconds from the reference boundaries to their nearest estimated ones, and the \emph{median estimation to reference} ($\sigma_{E2R}$) is analogous but swapping reference boundaries by estimated ones\cite{Pampalk}.
Median deviations are not commonly reported in publications of boundary algorithms since the hit rate tends to be more reliable (note that, unlike the F-measure of the hit rate, these median deviations are not normalized and not compacted in a single score).
However, they are, along with the hit rate, the only metrics included in MIREX to evaluate this task.

\subsection{Information Gain}

One of the most reliable metrics for beat tracking is the information gain\cite{Davies2009}. 
This score has successfully been used to identify hard tracks (in terms of beat tracking) from a machine point of view from a large dataset without reference annotations\cite{Holzapfel2012}.
This motivates us to explore its behavior when used for the task of boundary identification.

The idea behind this metric is to treat the error between the reference and the estimation as a probability distribution.
Typically, an error histogram of $K$ bins is used to capture these errors within a certain time range.
The information gain $D$ can be seen as the inverse of the entropy of the probability distribution defined by the error histogram $p(z)$. Formally:

\begin{equation}
  D = \log_2(K) - H(p(z))
\end{equation}

If $p(z_k)$ is the probability mass of the bin $k$ of the error histogram, then $H(\cdot)$, which is the entropy function, can be defined as follows:

\begin{equation}
  H(p(z)) = \sum_{k=1}^K p(z_k) \log_2 \left( \frac{1}{p(z_k)} \right)
\end{equation}

Therefore we obtain a high $D$ if the error distribution is a prominent peak in the histogram (i.e. the error does not vary), whereas $D$ is low, close to 0, if the histogram is uniform (i.e. the error has a high degree of variation and the entropy is high).
$D$ is bounded between 0 and $\log_2(K)$.

Typically, in the task of beat tracking, histograms of $K=41$ bins are used within the range of $\pm0.5$ seconds.
In our case, we use $K=250$ and an unrestricted time range in order to capture the higher errors that manifest in the boundary identification task compared to beat tracking.

\subsection{Trimming First and Last Boundaries}

The presented metrics treat all the boundaries of a given track equally. 
However, we can argue that the first and last boundaries are trivial to be retrieved (the first boundary should always be placed in time 0, and the last one in the time defined by the duration of the track). 
This topic was discussed in the music segmentation breaking session of last ISMIR edition\cite{Nieto2013}, and in a recent work by McFee only the trimmed scores are presented\cite{McFee2014}. 
Nonetheless, in this work we discuss the two versions of each given evaluation: trimmed and not trimmed.

\section{Machines Identifying Boundaries}\label{sec:eval_desc}

Three different approaches have been established when classifying music boundaries: novelty, homogeneity, and repetition\cite{Paulus2010}. 
Novelty-based approaches identify boundaries when there is a sudden change in some of the audio features (e.g. harmony, timbre).
Homogeneity algorithms analyze blocks of audio and extract segment boundaries based on the consistency of these blocks, again based on one or more audio features.
Finally, repetition-based methods aim to discover multiple occurrences of music patterns across the piece in order to identify the musical boundaries.

In this work we aim to compare various automatic approaches in order to have a good understanding of the capability of machines when segmenting music.
We select various segmentation algorithms and put them together in a Music Structural Analysis Framework (MSAF) to facilitate their execution, evaluation and analysis of results.

\subsection{Music Structure Analysis Framework}

A framework to compare various music segmentation algorithms, sharing their features and their evaluations, can be one of the most effective methodologies in order to explore and understand multiple algorithm behaviors.

\subsubsection{Audio Features}

To start with, we compute beat-synchronous audio features using Essentia\cite{Bogdanov2013}.
The harmonic features are Harmonic Pitch Class Profiles (HPCP) and the timbral features are Mel-Frequency Cepstral Coefficients (MFCC), both features computed using Essentia's default parameters.
The beats are detected using the multi-feature Essentia's beat tracker, and synchronized with the audio features using Ellis' method\cite{Ellis2007}.

\subsubsection{Algorithm Selection}

Two principles were applied when selecting the five algorithms to be used in this work: (i) The three different types of algorithms should be represented, and (ii) the reported results of these methods should be competitive when compared with the state of the art.
Note that this aims to emulate the behavior of having multiple human annotators segmenting the same tracks: not all of them will focus on the same type of boundaries (repetitive, homogeneous, novel), but we hope that all of them manage to retrieve fairly good quality boundaries in the end.

Based on the previous principles, we select five different algorithms:

\begin{itemize}
  \item
    \textbf{Foote}: One of the first music segmentation algorithms proposed, yet highly efficient and with good reported results\cite{Foote1999}. It is a novelty-based method that uses a Gaussian kernel across the diagonal of a self similarity matrix to identify sudden changes in the audio features.
  \item
    \textbf{Levy}: This approach uses Hidden Markov Model and constrained clustering to identify boundaries following a homogeneity-based principle\cite{Levy2008}.
  \item
    \textbf{Serr\`a}: This method defines the structural features, a set of features that combine both repeated and homogeneous boundaries. Once used to compute a novelty curve to extract the boundaries, this method combines the three different approaches to music segmentation, and reports some of the best scores for this task\cite{Serra2013}.
  \item
    \textbf{SI-PLCA}: Shift-Invariance Probabilistic Latent Component Analysis is an algorithm that uses a probabilistic variant of Non-negative Matrix Factorization in order to identify harmonic patterns with allowed key transpositions and time-shifted segments\cite{Weiss2011}. It can be seen as a repetition plus homogeneity-based approach.
  \item 
    \textbf{OLDA}: Ordinal Linear Discriminative Analysis uses the structural features defined in \cite{Serra2013} and applies a trained machine learning method (OLDA) to reduce their dimensionality and learn the most important set of features\cite{McFee2014}. This method obtains better results than Serr\`a's when using the hit rate at 0.5 seconds, and can be seen as an algorithm that also uses the three different principles of music segmentation.
\end{itemize}

\subsubsection{MSAF Performance}\label{subsub:performance}

We use the Isophonics Beatles dataset\footnote{http://isophonics.net/content/reference-annotations-beatles} in order to compare the performance of these five algorithms implemented in MSAF with their original publications. 
This dataset is composed by the 180 tracks published by The Beatles and it is commonly used to evaluate music segmentation methods using the F-measure of the hit rate described in section \ref{subsec:hitrate} with a 3 seconds window.
Some of these algorithms only accept harmonic features (e.g. SI-PLCA), and others are designed to accept a mixture of them (e.g. OLDA).

In Table \ref{tab:machine-algo-performance} a comparison between the reported results (R-F$_3$) and the actual performance obtained with our own implementations in MSAF (M-F$_T$) is shown.
Note that most of these methods reported better results in their publications.
We hypothesize that the decrease in performance arises from using slightly different parameters when computing the audio features, and/or by employing beat-synchronous features instead of standard frame-wise features.
Moreover, there is variation between the scores, especially when comparing the M-F$_3$ scores across algorithms, but we believe it is important to maintain a good representation of all boundary types to better capture the behavior of the machines (e.g. Levy's method is, to the best of our knowledge, the best homogeneous-based segmentation algorithm, but its scores, in the Isophonics Beatles, are not as good as the algorithms that aim to identify boundaries based on novelty only).

\begin{table}
 \begin{center}
 \begin{tabular}{|l|c|c|c|c|}
  \hline
  Algorithm & R-F$_3$ & M-F$_3$ & M-F$_{0.5}$ & Feature\\
  \hline
  Foote\tablefootnote{As reported in \cite{Kaiser2010}}     & 53.6 & 52.9 & 37.9 & MFCC\\
  Levy      & 54.0 & 48.5 & 26.3 & MFCC\\
  Serr\`a   & 77.4 & 66.5 & 32.5 & HPCP\\
  SI-PLCA\tablefootnote{As reported in \cite{Nieto2013}}   & 23.2 & 45.1 & 29.4 & HPCP\\
  OLDA      &  & 58.3 & 37.9 & Mix\\
  \hline
 \end{tabular}
\end{center}
 \caption{Performance of the MSAF in the Isophonics Beatles Dataset.}
 \label{tab:machine-algo-performance}
\end{table}

Levy, OLDA and SI-PLCA implementations are available on-line as open source projects written by their first authors.
Foote and Serr\`a methods were implemented from scratch specifically for this work.
MSAF is an open-source project, where anyone can contribute with their segmentation algorithms\footnote{Not displayed for revision process.}.

\subsection{Big Dataset}\label{sub:dataset}

In order to evaluate the algorithms in MSAF, we collected a large human annotated dataset of 2156 tracks that we refer to as \textbf{BIG} and is composed of four different datasets: Isophonics, SALAMI, Cerulean, and Epiphyte.

\begin{itemize}
  \item
    \textbf{Isophonics}\footnote{http://isophonics.net/datasets}: This is the most common human-annotated dataset to analyze segmentation algorithms. 
    It is composed of 300 tracks, including the 180 of The Beatles (discussed in \ref{subsub:performance}), of pop rock music by Queen, Michael Jackson, and others.

  \item
    \textbf{SALAMI}\cite{Smith2011}: The largest dataset with human-annotations for music segmentation published to date. 
    It is composed of more than 700 tracks, with music types including classic, folk, world music, jazz, blues, and rock.
    Some of these music genres can be particularly challenging to segment, both for machines and humans, as we will see in Section \ref{section:m-vs-h}.

  \item
    \textbf{Cerulean}: This dataset is a private annotation from a company that would like to remain anonymous.
    It is composed by 102 particularly challenging tracks, including progressive rock, death metal, or classical music.

  \item
    \textbf{Epiphyte}: This is another private dataset from yet another anonymous company.
    In this case over 1000 tracks were human-annotated, most of them being short pop rock and electronic songs.
    The intuition is that these should be fairly easy tracks to segment, due to their simple musical structures.

\end{itemize}

\subsection{Metrics Comparison}

We want to identify the tracks that machines find most difficult to segment.
In order to do so, we need to use a specific metric, and sort the tracks based on this score.
However, as we saw in Section \ref{sec:evalmetrics}, there are multiple metrics for boundary evaluation, so we first need to identify which one to use for sorting the tracks.

We use the Mean Mutual Agreement (MMA) and Mean Performance Ground-truth (MGP) approach as in \cite{Holzapfel2012}.
The MMA is the average of all the evaluations between algorithms (i.e. without using any human-annotated dataset).
In our case, there are 10 comparisons ($N(N-1)/2$, where $N=5$ segmentation algorithms) for each one of the metrics.
The MGP is the average performance of the 5 algorithms when compared with the human annotations.
By comparing the MMA with the MGP we can tell how well a specific metric behaves in order to choose the best for our purposes.
A histogram for each metric is computed by sorting the MMA results and placing their corresponding MGP scores in a 10 bin histogram to easily visualize how well the metric responds in all the different ranges of the score.

In Figure \ref{fig:machine-eval} we can see the histograms of the different metrics, sorted by their MMA value.
Analogously, in Figure \ref{fig:machine-eval-trim} we find the first and last boundary trimmed versions of the histograms.
As opposed to\cite{Holzapfel2012}, where beat trackers tend to agree most of the time, boundary algorithms usually perform far from perfectly.
Therefore, it is a hard task to tell whether a given track is hard to annotate solely by analyzing the MMA of a specific score.
We can see that the median deviations (MMA$_{\sigma_{R2E}}$ and MMA$_{\sigma_{E2R}}$, which are normalized using the maximum deviation) are mostly contained in a short range of the histogram, especially MMA$_{\sigma_{E2R}}$.
The information gain (MMA$_D$) is contained in a single histogram bin most of the time, making it difficult to predict hard tracks from just the MMA$_D$, since even in the worst MMA$_D$ track range, we find scores in their respective MPG$_D$ that fall in bins where only the easy tracks should be placed.
Finally, the MMA$_{F05}$ is too much spread across the histogram bins, with almost a flat spread curve.
However, the MMA$_{F3}$ seems to be the only metric that ranges almost all the histogram bins from hardest to easiest with a relatively narrow curve.

These metrics (or maybe the task itself) are still not able to predict hard and easy tracks from datasets without human-annotations, but the $F3$ metric qualitatively seems to behave the best of the ones tested here.
Therefore, we will use this evaluation in order to find the hardest and easiest tracks on our human-annotated dataset.

\begin{figure}
      \centering
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-F05.pdf}
              %\caption{Histogram of F-measure 0.5}
              \caption{}
              \label{fig:histo-F05}
      \end{subfigure}%
      ~ 
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-F3.pdf}
              \caption{}
              \label{fig:histo-F3}
      \end{subfigure}%
       
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-D.pdf}
              \caption{}
              \label{fig:histo-D}
      \end{subfigure}%
      ~ 
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-DevA2E.pdf}
              \caption{}
              \label{fig:histo-DevA2E}
      \end{subfigure}%
       
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-DevE2A.pdf}
              \caption{}
              \label{fig:histo-DevE2A}
      \end{subfigure}%

      \caption{Non-trimmed MSA vs MGP histograms}\label{fig:machine-eval}
\end{figure}

\begin{figure}
      \centering
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-F05-trim.pdf}
              %\caption{Histogram of F-measure 0.5}
              \caption{}
              \label{fig:histo-F05-trim}
      \end{subfigure}%
      ~ 
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-F3-trim.pdf}
              \caption{}
              \label{fig:histo-F3-trim}
      \end{subfigure}%
       
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-D-trim.pdf}
              \caption{}
              \label{fig:histo-D-trim}
      \end{subfigure}%
      ~ 
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-DevA2E-trim.pdf}
              \caption{}
              \label{fig:histo-DevA2E-trim}
      \end{subfigure}%
       
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-DevE2A-trim.pdf}
              \caption{}
              \label{fig:histo-DevE2A-trim}
      \end{subfigure}%

      \caption{Trimmed MSA vs MGP histograms}\label{fig:machine-eval-trim}
\end{figure}

\subsection{Hardest and Easiest Tracks}\label{sub:hard-easy}

We run our algorithms on our \textbf{BIG} dataset described in Section \ref{sub:dataset} and sort their tracks based on their MGP$_{F3}$ scores.
Thus, we are able to know which are the tracks of our dataset that machines find easiest and hardest to segment using the metric that coordinates the most with its respective MMA of the algorithms in MSAF.

In Figures \ref{fig:quartetto} and \ref{fig:promiscuous} we show examples of the boundaries of all the algorithms for a hard and an easy track, respectively, from a machine point of view.
The difficult track is a classical piece (string quartet) over 9 minutes long, and we can see the little agreement among algorithms when comparing their extracted boundaries.
The easy one is a popular dance song that is less than 3 minutes long, with a much higher degree of agreement.

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth, height=0.13\textheight]{plots/Quartetto-machine.pdf}
  \caption{One of the most difficult tracks to segment from a machine point of view.}
  \label{fig:quartetto}
\end{figure}%

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth, height=0.13\textheight]{plots/Promiscuous-machine.pdf}
  \caption{One of the tracks that machines find easiest to segment.}
  \label{fig:promiscuous}
\end{figure}%

The hardest tracks have a tendency of being longer in duration than the easier ones.
We can see this behavior in Figure \ref{fig:machinesdur}, where we see the tracks sorted by MGP$_{F3}$, which indicates that duration can have an impact in music segmentation from a machine point of view.

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth, height=0.13\textheight]{plots/machines-duration.pdf}
  \caption{Duration of all the tracks in the \textbf{BIG} dataset.}
  \label{fig:machinesdur}
\end{figure}%

%We can now proceed with a similar technique to identify the hardest and easiest tracks to segment from a human point of view.

\section{Humans Identifying Boundaries}\label{sec:using_method}

In this section we describe the process of segmenting music from a human perspective, and aim to identify the tracks that we, as humans, find hard (or easy) to segment. 
To do so we create a smaller dataset that is used to run an experiment and then analyze the results using similar methods as described in the previous section.

\subsection{Small Dataset}

Using the technique described in Section \ref{sub:hard-easy}, we create a small dataset that we refer as \textbf{SMA} by selecting the hardest 45 tracks and the easiest 5 from the \textbf{BIG} dataset that meet the following requirements: (i) they are no longer than 10 minutes long, and (ii) they are actual music pieces (in SALAMI we can find some tracks that are only speech fragments).

We filtered 6 tracks that agreed with one of these two principles in the hardest 45 (2 tracks were over 10 minutes and 4 tracks were only speech). 
Moreover, the average length for each song for the entire dataset is 244.1 seconds, whereas the average track length for the 45 hardest tracks selected (once we filtered the ones that met the defined criteria) is 348.5 seconds.
The 45 hardest tracks are composed by 8 tracks from the Cerulean dataset, 32 from the SALAMI dataset, and 5 from the Isophonics set.
It is interesting to note that no Epiphyte track was selected as difficult, even when almost half of the tracks in the \textbf{BIG} dataset are from the Epiphyte subset.
On the other hand, the easiest 5 tracks were composed of 4 Epiphyte tracks and 1 Isophonics.
This shows how the metric MGP$_{F3}$ is qualitatively working as expected.


\subsection{Experiment Setup}

We asked 5 music graduate students to identify the boundaries of the \textbf{SMA} dataset.
These subjects had an average of 11 years of musical training, even though it has been shown that the perception of boundaries in music does not depend on the musical background of the participants\cite{Bruderer2009}.
We asked the subjects to identify the boundaries, following the same guidelines as the ones used when collecting the data for SALAMI\cite{Smith2011}.
The participants used the Sonic Visualiser\cite{Cannam2006} in order to segment the 50 tracks, and for all they found difficult to segment, they had to describe why by using at least one musical/acoustic adjective.

\subsection{Analysis of the Results}


\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth, height=0.13\textheight]{plots/Quartetto-human.pdf}
  \caption{One of the most difficult tracks to segment from a human point of view.}
  \label{fig:quartetto-human}
\end{figure}%

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth, height=0.13\textheight]{plots/Promiscuous-human.pdf}
  \caption{One of the tracks that humans find easiest to segment.}
  \label{fig:promiscuous-human}
\end{figure}%

\section{Machines vs Humans}\label{section:m-vs-h}

Plot comparing MGP-machines vs MGP-humans.

Analyze the answers, display most common problems in a table.

%\begin{table}
 %\begin{center}
 %\begin{tabular}{|l|l|}
  %\hline
  %String value & Numeric value \\
  %\hline
  %Hello ISMIR  & 2014 \\
  %\hline
 %\end{tabular}
%\end{center}
 %\caption{Table captions should be placed below the table.}
 %\label{tab:example}
%\end{table}

%\begin{figure}
 %\centerline{\framebox{
 %\includegraphics[width=\columnwidth]{figure.png}}}
 %\caption{Figure captions should be placed below the figure.}
 %\label{fig:example}
%\end{figure}

\section{Conclusions}

Identified some audio properties that could improve the automatic segmentation algorithms.


%\begin{thebibliography}{citations}

%\bibitem {Author:00}
%E. Author:
%``The Title of the Conference Paper,''
%{\it Proceedings of the International Symposium
%on Music Information Retrieval}, pp.~000--111, 2000.

%\bibitem{Someone:10}
%A. Someone, B. Someone, and C. Someone:
%``The Title of the Journal Paper,''
%{\it Journal of New Music Research},
%Vol.~A, No.~B, pp.~111--222, 2010.

%\bibitem{Someone:04} X. Someone and Y. Someone: {\it Title of the Book},
    %Editorial Acme, Porto, 2012.

%\end{thebibliography}

\small
\bibliography{references}

\end{document}
