% -----------------------------------------------
% Template for ISMIR 2014
% (based on earlier ISMIR templates)
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir2014,amsmath,cite}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tablefootnote}

% Title.
% ------
\title{Analysis of Music Segmentation Boundaries:\\ Machines vs Humans}

% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
  %{First author} {School \\ Department}
  %{Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
  The automatic identification of segment boundaries in music has had a strong relevance in the field of Music Information Retrieval in the recent years.
  This task presents many challenges, and arguably the most notable one is the subjectivity of the task itself: two humans do not always agree on the same set of boundaries.
  In this work we present an analysis of this task from two different points of view: machines and humans.
  Five algorithms were selected to segment a dataset of more than 2000 tracks.
  Then we collected multiple human annotations for the 45 hardest tracks and the 5 easiest tracks from a machine point of view.
  The results show that humans' mutual agreement is much higher than machines'.
  We explore the hardest tracks from a machine point of view and the easiest tracks from a human point of view in order to identify the set of audio properties that could potentially improve the current boundary identification algorithms.
  %We present a comparison that aims to shed light into the shortcomings of the current evaluation metrics and discuss a list of properties in the audio that make this task particularly difficult from a machine point of view while humans can 
  
\end{abstract}
%
\section{Introduction}\label{sec:introduction}

Music segmentation intro: focusing on boundaries.

Subjectivity of the task.

Analysis of how machines do vs humans. Aim to improve automatic algorithms.

Organization of the paper.

\section{Boundaries Evaluation Metrics}

The boundaries of a given track are usually evaluated using the hit rate metric.
In this section we review the hit rate, discuss its shortcomings, and explore other metrics that might help addressing our project.
We refer the human annotated boundaries as \emph{reference}, and the ones identified by an algorithm as \emph{estimated}.

\subsection{Hit Rate}\label{subsec:hitrate}

The hit rate for boundaries evaluation is the most common and established metric for this task and it is traditionally used with a specific time window of 3 and/or 0.5 seconds\cite{Ong2005}. 

This metric considers an estimated boundary as \emph{correct} (a hit) if it falls under the specified time window centered at its corresponding reference boundary.
These hits are then used to compute three scores: the Precision (number of hits over the number of estimated boundaries), the Recall (number of hits over the number of reference boundaries), and the F-measure (harmonic mean between Precision and Recall), which is computed as follows:

\begin{equation}
  F = 2 \frac{P R}{P + R}
\end{equation}

\subsection{Median Deviations}

This metric involves the computation of two median deviations: \emph{median reference to estimation} is the median in seconds from the reference boundaries to their nearest estimated ones, and the \emph{median estimation to reference} is analogous but swapping reference boundaries by estimated ones\cite{Pampalk}.
Median deviations are not commonly reported in publications of boundary algorithms since the hit rate tends to be more reliable (note that these median deviations are not normalized and not compacted in a single score like the F-measure).
However, they are, along with the hit rate, the only metrics included in MIREX to evaluate estimated boundaries.

\subsection{Information Gain}

One of the most reliable metrics for beat tracking is the information gain\cite{Davies2009}. 
This score has successfully been used to identify hard tracks (in terms of beat tracking) from a machine point of view from a large dataset without reference annotations\cite{Holzapfel2012}.
This motivates us to explore its behavior when used for the task of boundary identification.

The idea behind this metric is to treat the error between the reference and the estimation as a probability distribution.
Typically, an error histogram of $K$ bins is used to capture these errors within a certain time range.
The information gain $D$ can be seen as the inverse of the entropy of the probability distribution defined by the error histogram $p(z)$. Formally:

\begin{equation}
  D = \log_2(K) - H(p(z))
\end{equation}

If $p(z_k)$ is the probability mass of the bin $k$ of the error histogram, then $H(\cdot)$, which is the entropy function, can be defined as follows:

\begin{equation}
  H(p(z)) = \sum_{k=1}^K p(z_k) \log_2 \left( \frac{1}{p(z_k)} \right)
\end{equation}

Therefore we obtain a high $D$ if the error distribution is a prominent peak in the histogram (i.e. the error does not vary), whereas $D$ is low, close to 0, if the histogram is uniform (i.e. the error has a high degree of variation and the entropy is high).
$D$ is bounded between 0 and $\log_2(K)$.

Typically, in the task of beat tracking, histograms of $K=41$ bins are used within the range of $\pm0.5$ seconds.
In our case, we use $K=250$ and an unrestricted time range in order to capture the higher errors that manifest in the boundary identification task compared to beat tracking.

\subsection{Trimming First and Last Boundaries}

The presented metrics treat all the boundaries of a given track equally. 
However, we can argue that the first and last boundaries are trivial to be retrieved (the first boundary should always be placed in second 0, and the last one in the second defined by the duration of the track). 
This topic was discussed in the music segmentation breaking sessions of last ISMIR edition\cite{Nieto2013}, and in this work we present two versions of each given evaluation: trimmed and not trimmed.

\section{Machines Identifying Boundaries}\label{sec:eval_desc}

Three different approaches have been established when classifying music segmentation algorithms: novelty, homogeneity, and repetition-based methods\cite{Paulus2010}. 
Novelty-based approaches identify boundaries when there is a sudden change in some of the audio features (e.g. harmony, timbre).
Homogeneity algorithms analyze blocks of audio and extract segment boundaries based on the consistency of these blocks, again based on one or more audio features.
Finally, repetition-based methods aim to discover multiple occurrences of music patterns across the piece in order to identify the musical boundaries.

In this work we aim to compare various automatic approaches in order to have a good understanding on the capability of machines when segmenting music.
We select various segmentation algorithms and put them together in a Music Structural Analysis Framework (MSAF) to facilitate their evaluation and analysis of results.

\subsection{Music Structure Analysis Framework}

A framework to compare various music segmentation algorithms, sharing their features and their evaluations, can be one of the most effective methodologies in order to explore and understand multiple algorithm behaviors.

\subsubsection{Audio Features}

To start with, we compute beat-synchronous audio features using Essentia\cite{Bogdanov2013}.
The harmonic features are Harmonic Pitch Class Profiles (HPCP) and the timbral features are Mel-Frequency Cepstral Coefficients (MFCC), both features computed using Essentia's default parameters.
The beats are detected using the multi-feature Essentia's beat tracker, and synchronized with the audio features using Ellis' method\cite{Ellis2007}.

\subsubsection{Algorithm Selection}

Two principles were applied when selecting the five algorithms to be used in this work: (i) The three different types of algorithms should be represented, and (ii) the reported results of these methods should be competitive when compared with the state of the art.
Note that this aims to emulate the behavior of having multiple human annotators segmenting the same tracks: not all of them will focus on the same type of boundaries (repetitive, homogeneous, novel), but we hope that all of them manage to retrieve fairly good quality boundaries in the end.

Based on the previous principles, we select five different algorithms:

\begin{itemize}
  \item
    \textbf{Foote}: One of the first music segmentation algorithms proposed, yet highly efficient and with good reported results\cite{Foote1999}. Novelty-based method that uses a Gaussian kernel across the diagonal of a self similarity matrix to identify sudden changes in the audio features.
  \item
    \textbf{Levy}: This approach uses Hidden Markov Model and constrained clustering to identify boundaries following a Homogeneity-based principle\cite{Levy2008}.
  \item
    \textbf{Serr\`a}: This method defines the structural features, a set of features that combine both repeated and homogeneous boundaries. Once used to compute a novelty curve to extract the boundaries, this method combines the three different approaches to music segmentation, and reports some of the best scores for this task\cite{Serra2013}.
  \item
    \textbf{SI-PLCA}: Shift-Invariance Probabilistic Latent Component Analysis is an algorithm that uses a probabilistic variant of Non-negative Matrix Factorization in order to identify harmonic patterns with allowed key transpositions and time-shifted segments\cite{Weiss2011}. It can be seen as a repetition plus homogeneity-based approach.
  \item 
    \textbf{OLDA}: Ordinal Linear Discriminative Analysis uses the structural features defined in \cite{Serra2013} and applies a trained machine learning method (LDA) to reduce their dimensionality and learn the most important set of features\cite{McFee2014}. This method obtains better results than Serr\`a's when using the hit rate at 0.5 seconds, and can be seen as an algorithm that also uses the three different principles of music segmentation.
\end{itemize}

\subsubsection{Algorithm Evaluation}

We use the Isophonics Beatles dataset\footnote{http://isophonics.net/content/reference-annotations-beatles} in order to compare the performance of these five algorithms with their original publications. 
This dataset is composed by the 180 tracks published by The Beatles and it is commonly used to evaluate music segmentation methods using the F-measure of the hit rate described in section \ref{subsec:hitrate} with a 3 seconds window.
Some of these algorithms only accept harmonic features (e.g. SI-PLCA), and others are designed to accept a mixture of them (e.g. OLDA).

In Table \ref{tab:machine-algo-performance} a comparison between the reported results (R-F$_3$) and the actual performance obtained with our own implementations in MSAF (M-F$_T$) is shown.
Note that most of these methods reported better results in their publications.
We hypothesize that the decrease in performance arises from using slightly different parameters when computing the audio features, or by employing beat-synchronous features instead of standard frame-wise features.
There is variation between the scores, especially when comparing the M-F$_3$ scores, but we believe it is important to maintain a good representation of all boundary types to be found when understanding the behavior of the machines (e.g. Levy's method is, to the best of our knowledge, the best homogeneous-based segmentation algorithm).

Levy, OLDA and SI-PLCA implementations are available on-line as open source projects.
Foote and Serr\`a methods were implemented from scratch.
MSAF is an open-source project, where anyone can contribute with their segmentation algorithms\footnote{Not displayed for revision process.}.

\begin{table}
 \begin{center}
 \begin{tabular}{|l|c|c|c|c|}
  \hline
  Algorithm & R-F$_3$ & M-F$_3$ & M-F$_{0.5}$ & Feature\\
  \hline
  Foote\tablefootnote{As reported in \cite{Kaiser2010}}     & 53.6 & 52.9 & 37.9 & MFCC\\
  Levy      & 54.0 & 48.5 & 26.3 & MFCC\\
  Serr\`a   & 77.4 & 66.5 & 32.5 & HPCP\\
  SI-PLCA\tablefootnote{As reported in \cite{Nieto2013}}   & 23.2 & 45.1 & 29.4 & HPCP\\
  OLDA      &  & 58.3 & 37.9 & Mix\\
  \hline
 \end{tabular}
\end{center}
 \caption{Table captions should be placed below the table.}
 \label{tab:machine-algo-performance}
\end{table}

\subsection{Dataset}

Isophonics, SALAMI\cite{Smith2011}, Cerulean, Ephiphyte.

\subsection{Metrics Comparison}

Talk about the Mean Mutual Agreement and Mean Performance Ground-truth\cite{Holzapfel2012}.

\begin{figure}
      \centering
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-F05.pdf}
              %\caption{Histogram of F-measure 0.5}
              \caption{}
              \label{fig:histo-F05}
      \end{subfigure}%
      ~ 
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-F3.pdf}
              \caption{}
              \label{fig:histo-F3}
      \end{subfigure}%
       
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-D.pdf}
              \caption{}
              \label{fig:histo-D}
      \end{subfigure}%
      ~ 
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-DevA2E.pdf}
              \caption{}
              \label{fig:histo-DevA2E}
      \end{subfigure}%
       
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-DevE2A.pdf}
              \caption{}
              \label{fig:histo-DevE2A}
      \end{subfigure}%

      \caption{Non-trimmed Evaluations}\label{fig:machine-eval}
\end{figure}

\begin{figure}
      \centering
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-F05-trim.pdf}
              %\caption{Histogram of F-measure 0.5}
              \caption{}
              \label{fig:histo-F05-trim}
      \end{subfigure}%
      ~ 
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-F3-trim.pdf}
              \caption{}
              \label{fig:histo-F3-trim}
      \end{subfigure}%
       
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-D-trim.pdf}
              \caption{}
              \label{fig:histo-D-trim}
      \end{subfigure}%
      ~ 
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-DevA2E-trim.pdf}
              \caption{}
              \label{fig:histo-DevA2E-trim}
      \end{subfigure}%
       
      \begin{subfigure}[b]{0.25\textwidth}
              \includegraphics[width=\textwidth]{plots/histo-DevE2A-trim.pdf}
              \caption{}
              \label{fig:histo-DevE2A-trim}
      \end{subfigure}%

      \caption{Trimmed Evaluations}\label{fig:machine-eval-trim}
\end{figure}
%\begin{figure}
      %\centering
      %\begin{subfigure}[b]{0.25\textwidth}
              %\includegraphics[width=\textwidth]{plots/histo-F05.pdf}
              %%\caption{Histogram of F-measure 0.5}
              %\caption{}
              %\label{fig:histo-F05}
      %\end{subfigure}%
      %~ 
      %\begin{subfigure}[b]{0.25\textwidth}
              %\includegraphics[width=\textwidth]{plots/correl-F05.pdf}
              %\caption{}
              %\label{fig:correl-F05}
      %\end{subfigure}

      %\begin{subfigure}[b]{0.25\textwidth}
              %\includegraphics[width=\textwidth]{plots/histo-F3.pdf}
              %\caption{}
              %\label{fig:histo-F3}
      %\end{subfigure}%
      %~ 
      %\begin{subfigure}[b]{0.25\textwidth}
              %\includegraphics[width=\textwidth]{plots/correl-F3.pdf}
              %\caption{}
              %\label{fig:correl-F3}
      %\end{subfigure}

      %\begin{subfigure}[b]{0.25\textwidth}
              %\includegraphics[width=\textwidth]{plots/histo-D.pdf}
              %\caption{}
              %\label{fig:histo-D}
      %\end{subfigure}%
      %~ 
      %\begin{subfigure}[b]{0.25\textwidth}
              %\includegraphics[width=\textwidth]{plots/correl-D.pdf}
              %\caption{}
              %\label{fig:correl-D}
      %\end{subfigure}

      %\begin{subfigure}[b]{0.25\textwidth}
              %\includegraphics[width=\textwidth]{plots/histo-DevA2E.pdf}
              %\caption{}
              %\label{fig:histo-DevA2E}
      %\end{subfigure}%
      %~ 
      %\begin{subfigure}[b]{0.25\textwidth}
              %\includegraphics[width=\textwidth]{plots/correl-DevA2E.pdf}
              %\caption{}
              %\label{fig:correl-DevA2E}
      %\end{subfigure}

      %\begin{subfigure}[b]{0.25\textwidth}
              %\includegraphics[width=\textwidth]{plots/histo-DevE2A.pdf}
              %\caption{}
              %\label{fig:histo-DevE2A}
      %\end{subfigure}%
      %~ 
      %\begin{subfigure}[b]{0.25\textwidth}
              %\includegraphics[width=\textwidth]{plots/correl-DevE2A.pdf}
              %\caption{}
              %\label{fig:correl-DevE2A}
      %\end{subfigure}

      %\caption{Pictures of animals}\label{fig:animals}
%\end{figure}

\subsection{Selection of Hardest and Easiest Tracks}

Plots of MGP and MMA.

\section{Humans Identifying Boundaries}\label{sec:using_method}

Experiment. 5 subjects. 50 tracks: 45 hard, 5 easy.

\subsection{Experiment Setup}

Talk about the SALAMI guidelines.

\section{Machines vs Humans}

Plot comparing MGP-machines vs MGP-humans.

Analyze the answers, display most common problems in a table.

%\begin{table}
 %\begin{center}
 %\begin{tabular}{|l|l|}
  %\hline
  %String value & Numeric value \\
  %\hline
  %Hello ISMIR  & 2014 \\
  %\hline
 %\end{tabular}
%\end{center}
 %\caption{Table captions should be placed below the table.}
 %\label{tab:example}
%\end{table}

%\begin{figure}
 %\centerline{\framebox{
 %\includegraphics[width=\columnwidth]{figure.png}}}
 %\caption{Figure captions should be placed below the figure.}
 %\label{fig:example}
%\end{figure}

\section{Conclusions}

Identified some audio properties that could improve the automatic segmentation algorithms.


%\begin{thebibliography}{citations}

%\bibitem {Author:00}
%E. Author:
%``The Title of the Conference Paper,''
%{\it Proceedings of the International Symposium
%on Music Information Retrieval}, pp.~000--111, 2000.

%\bibitem{Someone:10}
%A. Someone, B. Someone, and C. Someone:
%``The Title of the Journal Paper,''
%{\it Journal of New Music Research},
%Vol.~A, No.~B, pp.~111--222, 2010.

%\bibitem{Someone:04} X. Someone and Y. Someone: {\it Title of the Book},
    %Editorial Acme, Porto, 2012.

%\end{thebibliography}

\bibliography{references}

\end{document}
